{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation\n",
    "======\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance\n",
    "========\n",
    "\n",
    "* $\\operatorname{Var}(X) = \\operatorname{E}\\left[(X - \\mu)^2 \\right] = \n",
    "\\int (X - \\mu)^2 dF(x)$\n",
    "\n",
    "* $\\operatorname{Var}(X) = \\operatorname{Cov}(X, X)$\n",
    "\n",
    "* $$\n",
    "\\begin{align}\n",
    "\\operatorname{Var}(X) &= \\operatorname{E}\\left[(X - \\operatorname{E}[X])^2\\right] \\\\\n",
    "&= \\operatorname{E}\\left[X^2 - 2X\\operatorname{E}[X] + (\\operatorname{E}[X])^2\\right] \\\\\n",
    "&= \\operatorname{E}\\left[X^2\\right] - 2\\operatorname{E}[X]\\operatorname{E}[X] + (\\operatorname{E}[X])^2 \\\\\n",
    "&= \\operatorname{E}\\left[X^2 \\right] - (\\operatorname{E}[X])^2\n",
    "\\end{align}\n",
    "$$\n",
    "That is, $E\\left[X^2\\right] - (E[X])^2$, which can be remembered as \"mean of square minus square of mean\".\n",
    "\n",
    "* For continuous random variables,  \n",
    "$\\operatorname{Var}(X) =\\sigma^2 =\\int (x-\\mu)^2 \\, f(x) \\, dx\\, =\\int x^2 \\, f(x) \\, dx\\, - \\mu^2$\n",
    "If expected value doesn't exist, so does the variance.  \n",
    "Ex. \n",
    "  * Cauchy distribution\n",
    "  * Pareto distribution with $k \\in (1, 2]$\n",
    "\n",
    "* For discrete random variables,  \n",
    "$$\n",
    "\\operatorname{Var}(X) = \\sum_{i=1}^n p_i\\cdot(x_i - \\mu)^2 = \\sum_{i=1}^n p_i x_i ^2- \\mu^2,\n",
    "$$\n",
    "  * If the $X_i \\sim F$ for some F, then  \n",
    "  $$ \\operatorname{Var}(X) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2. \\text{where } \\mu = \\frac{1}{n}\\sum_{i=1}^n x_i $$\n",
    "  * The mean in the formula can be avoided as:  \n",
    "  $$ \\operatorname{Var}(X) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\frac{1}{2}(x_i - x_j)^2 = \\frac{1}{n^2}\\sum_i \\sum_{j>i} (x_i-x_j)^2.$$\n",
    "  \n",
    "* Transformations:  \n",
    "  * **Offset: ** $\\operatorname{Var}(X+a) = \\operatorname{Var}(X)$\n",
    "  * **Scaling**: $\\operatorname{Var}(aX) = a^2 \\operatorname{Var}(X)$\n",
    "  * $\\operatorname{Var}(aX+bY)=a^2\\operatorname{Var}(X)+b^2\\operatorname{Var}(Y)+2ab\\, \\operatorname{Cov}(X,Y)$\n",
    "  * $\\operatorname{Var}(aX-bY)=a^2\\operatorname{Var}(X)+b^2\\operatorname{Var}(Y)-2ab\\, \\operatorname{Cov}(X,Y)$\n",
    "  * **Sum**: $\\operatorname{Var}\\left(\\sum_{i=1}^N X_i\\right)=\\sum_{i,j=1}^N\\operatorname{Cov}(X_i,X_j)\\\\=\\sum_{i=1}^N\\operatorname{Var}(X_i)+\\sum_{i\\ne j}\\operatorname{Cov}(X_i,X_j).$\n",
    "  * **Linear combination**: $$\n",
    "    \\begin{align}\n",
    "\\operatorname{Var}\\left( \\sum_{i=1}^N a_iX_i\\right) &=\\sum_{i,j=1}^{N} a_ia_j\\operatorname{Cov}(X_i,X_j) \\\\\n",
    "&=\\sum_{i=1}^N a_i^2\\operatorname{Var}(X_i)+\\sum_{i\\not=j}a_ia_j\\operatorname{Cov}(X_i,X_j)\\\\\n",
    "& =\\sum_{i=1}^N a_i^2\\operatorname{Var}(X_i)+2\\sum_{1\\le i<j\\le N}a_ia_j\\operatorname{Cov}(X_i,X_j).\n",
    "\\end{align}\n",
    "    $$\n",
    "    \n",
    "* Uncorrelated RVs:  \n",
    "  * If $\\operatorname{Cov}(X_i,X_j)=0\\ ,\\ \\forall\\ (i\\ne j)$, then $\\{X_i\\}$ are uncorrelated. This then leads to  \n",
    "  $\\operatorname{Var}\\left(\\sum_{i=1}^N X_i\\right)=\\sum_{i=1}^N\\operatorname{Var}(X_i)$\n",
    "  * If RVs are independent, then they are uncorrelated. Note that, independence is sufficient but not necessary for the variance of the sum to equal the sum of the variances.\n",
    "  * $$\\operatorname{Var}\\left(\\overline{X}\\right) = \\operatorname{Var}\\left(\\frac {1} {n}\\sum_{i=1}^n X_i\\right) = \\frac {1} {n^2}\\sum_{i=1}^n \\operatorname{Var}\\left(X_i\\right) = \\frac {\\sigma^2} {n}$$\n",
    "  \n",
    "* Sum of correlated variables  \n",
    "  * If the RVs have equal variance $\\sigma^2$ and the average correlation of the distinct variables is $\\rho$, then,  \n",
    "  $$\\operatorname{Var}(\\overline{X}) = \\frac {\\sigma^2} {n} + \\frac {n-1} {n} \\rho \\sigma^2$$\n",
    "  * Thus, variance of the mean increases with the average of the correlations.\n",
    "  * In other words, additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean.\n",
    "  * Convergence:  \n",
    "    * $\\lim_{n \\to \\infty} \\operatorname{Var}(\\overline{X}) = \\rho \\sigma^2$\n",
    "    * This makes clear that the sample mean of correlated variables does not generally converge to the population mean, even though the [Law of large numbers](http://www.wikiwand.com/en/Law_of_large_numbers) states that the sample mean will converge for independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bessel's Correction\n",
    "=====================\n",
    "\n",
    "Sample Variance = $\\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\mu)^2$\n",
    "\n",
    "**Caveats**  \n",
    "* It does not yield an unbiased estimator of standard deviation.\n",
    "  * while the sample variance (using Bessel's correction) is an unbiased estimate of the population variance, its square root, the sample standard deviation, is a biased estimate of the population standard deviation; because the square root is a concave function, the bias is downward, by Jensen's inequality. There is no general formula for an unbiased estimator of the population standard deviation, though there are correction factors for particular distributions, such as the normal; see unbiased estimation of standard deviation for details. An approximation for the exact correction factor for the normal distribution is given by using n − 1.5 in the formula: the bias decays quadratically (rather than linearly, as in the uncorrected form and Bessel's corrected form).\n",
    "  \n",
    "* The corrected estimator often has worse (higher) mean squared error (MSE) than the uncorrected estimator, and never has the minimum MSE\n",
    "  * A different scale factor can always be chosen to minimize MSE. The optimal value depends on excess kurtosis, as discussed in [mean squared error: variance](http://www.wikiwand.com/en/Mean_squared_error#/Variance). For the normal distribution this is optimized by dividing by n + 1 (instead of n − 1 or n).\n",
    "* It is only necessary when the population mean is unknown (and estimated as the sample mean).\n",
    "\n",
    "\n",
    "**Explanation 1**\n",
    "\n",
    "One can understand Bessel's correction intuitively as the degrees of freedom in the residuals vector (residuals, not errors, because the population mean is unknown):\n",
    "\n",
    "$$(x_1-\\overline{x},\\,\\dots,\\,x_n-\\overline{x})$$\n",
    "\n",
    "where $\\overline{x}$ is the sample mean. While there are n independent samples, there are only n − 1 independent residuals, as they sum to 0.\n",
    "\n",
    "**Explanation 2**\n",
    "\n",
    "* Comes from [Jensen's inequality](http://www.wikiwand.com/en/Jensen%27s_inequality)\n",
    "\n",
    "* Say, the population mean, $\\mu$ is 2050\n",
    "* Samples: $x_i$ = 2051, 2053, 2055, 2050, 2051\n",
    "* Sample mean $\\hat{\\mu} = \\frac{\\sum_{i} x_i}{5} = 2052$\n",
    "* If we use the population mean (2050), the variance is,\n",
    "$var(x; \\mu) = \\frac{\\sum_{i=1}^{5} (x_i - \\mu)^2}{5} = \\frac{\\sum_{i=1}^{5} (x_i - 2050)^2}{5}  = \\frac{36}{5} = 7.2$\n",
    "* If we use the sample mean (2052), the variance is,\n",
    "$var(x; \\hat{\\mu}) = \\frac{\\sum_{i=1}^{5} (x_i - \\hat{\\mu})^2}{5} = \\frac{\\sum_{i=1}^{5} (x_i - 2052)^2}{5}  = \\frac{16}{5} = 3.2$\n",
    "\n",
    "* Is the variance computed using sample mean always smaller than if we used the population mean?\n",
    "\n",
    "* We will see what happens at a single point. Say we use the population mean (2050) to compute the variance.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " [ \\underbrace{2053-2050}_{\n",
    "             \\begin{smallmatrix}\n",
    "                 \\text{Deviation from}\\\\\n",
    "                 \\text{population mean}\n",
    "             \\end{smallmatrix}\n",
    "        }\n",
    " ]^2 \n",
    " &=  [ \\overbrace{\n",
    "             \\underbrace{(2053 - 2052)}_{\n",
    "                \\begin{smallmatrix}\n",
    "                    \\text{Deviation from} \\\\ \n",
    "                    \\text{sample mean}\n",
    "                 \\end{smallmatrix}\n",
    "              }\n",
    "          }^{\\text{This is a.}}  + \n",
    " \\overbrace{(2052 - 2050)}^{\\text{This is }b.}\\,]^2 \\\\\n",
    " & = \\overbrace{(2053 - 2052)^2}^{\\text{This is }a^2.} +\n",
    "     \\overbrace{2(2053 - 2052)(2052 - 2050)}^{\\text{This is }2ab.} +\n",
    "     \\overbrace{(2052 - 2050)^2}^{\\text{This is }b^2.}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* Now extending to all points  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\overbrace{(2051 - 2052)^2}^{\\text{This is }a^2.} &\n",
    "  +\\  \\overbrace{2(2051 - 2052)(2052 - 2050)}^{\\text{This is }2ab.} &\n",
    "  +\\  \\overbrace{(2052 - 2050)^2}^{\\text{This is }b^2.} \\\\\n",
    "  (2053 - 2052)^2\\  &+\\  2(2053 - 2052)(2052 - 2050)\\  &+\\ (2052 - 2050)^2 \\\\\n",
    "  (2055 - 2052)^2\\  &+\\  2(2055 - 2052)(2052 - 2050)\\  &+\\ (2052 - 2050)^2 \\\\\n",
    "  (2050 - 2052)^2\\  &+\\  2(2050 - 2052)(2052 - 2050)\\  &+\\ (2052 - 2050)^2 \\\\\n",
    "  (2051 - 2052)^2\\  &\n",
    "  +\\  \\underbrace{2(2051 - 2052)(2052 - 2050)}_{\n",
    "          \\begin{smallmatrix}\n",
    "              \\text{The sum of the entries in this} \\\\\n",
    "              \\text{middle column must be 0.}\n",
    "           \\end{smallmatrix}}\\ &\n",
    "   +\\  (2052 - 2050)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* For the second column, notice that the term (2052-2050) is a constant. Hence, the sum of second column is the sum of its first terms times this constant. But 2052 is the sample mean. Hence, the sum of the second column becomes zero\n",
    "* The first column is the variance computed using the sample mean\n",
    "* The third column is the residue, each of which is $(\\mu - \\hat{\\mu})^2$\n",
    "* Thus $var(x; \\mu) = var(x; \\hat{\\mu}) + 5 (\\mu - \\hat{\\mu})^2$. The second term will only if the sample mean is equal to the population mean. Else, it is always a positive value (since it is a sum of squares) and leads to underestimation of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof for correction**  \n",
    "\n",
    "Idea:\n",
    "\n",
    "In the biased estimator, by using the sample mean instead of the true mean, you are underestimating each $x_i − \\mu$ by $\\overline{x} − \\mu$. We know that the variance of a sum is the sum of the variances (for uncorrelated variables). So, to find the discrepancy between the biased estimator and the true variance, we just need to find the variance of $\\overline{x} − \\mu$. This is just the [variance of the sample mean](http://www.wikiwand.com/en/Variance#/Sum_of_uncorrelated_variables_.28Bienaym.C3.A9_formula.29), which is $\\sigma^2/n$. So, we expect that the biased estimator underestimates $\\sigma^2$ by $\\sigma^2/n$, and so the biased estimator = (1 − 1/n) × the unbiased estimator = (n − 1)/n × the unbiased estimator.\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "E \\left[ \\sigma^2 - s_{biased}^2 \\right] &\n",
    "= E\\left[ \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2 - \\frac{1}{n}\\sum_{i=1}^n (x_i - \\overline{x})^2 \\right] \\\\\n",
    "&= \\frac{1}{n} E\\left[ \\sum_{i=1}^n\\left((x_i^2 - 2 x_i \\mu + \\mu^2) - (x_i^2 - 2 x_i \\overline{x} + \\overline{x}^2)\\right) \\right] \\\\\n",
    "&= \\frac{1}{n} E\\left[ \\sum_{i=1}^n\\left( - 2 x_i \\mu + \\mu^2 + 2 x_i \\overline{x} - \\overline{x}^2\\right) \\right] \\\\\n",
    "&= E\\left[ - 2 \\overline{x} \\mu + \\mu^2 + 2 \\overline{x}^2 - \\overline{x}^2 \\right] \\\\\n",
    "&= E\\left[  \\mu^2 - 2 \\overline{x} \\mu + \\overline{x}^2 \\right] \\\\\n",
    "&= E\\left[  (\\overline{x}   - \\mu)^2 \\right] \\\\\n",
    "&= \\text{Var} (\\overline{x}) \\\\\n",
    "&= \\frac{\\sigma^2}{n}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence,  \n",
    "$\n",
    "\\operatorname{E} \\left[ s^2_{\\text{biased}} \\right] = \n",
    "\\sigma^2 - \\frac{\\sigma^2}{n} =\n",
    "\\frac{n-1}{n} \\sigma^2 \n",
    "$\n",
    "\n",
    "So, an unbiased estimator should be given by\n",
    " $ s_{\\text{unbiased}}^2 = \\frac{n}{n-1} s_{\\text{biased}}^2 = \\frac{1}{n-1} \\sum (x_i - \\overline{x})^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Variance\n",
    "==========\n",
    "\n",
    "Unbiased estimate:\n",
    "$$\n",
    "\\sigma^2 = \\displaystyle\\frac {\\sum_{i=1}^n x_i^2 - (\\sum_{i=1}^n x_i)^2/n}{n-1}.\n",
    "$$\n",
    "\n",
    "the terms in the numerator can be similar numbers, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": false,
    "deletable": true,
    "locked": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 5 2 1 0 2 0 0 1]\n",
      "[  886.          1447.21666667  2008.43333333  2569.65        3130.86666667\n",
      "  3692.08333333  4253.3         4814.51666667  5375.73333333  5936.95\n",
      "  6498.16666667]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5e84090cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = 10^5 # Number of trials\n",
    "def compute_diff():\n",
    "    N = np.random.randint(low=10, high=100) # no. of samples each time\n",
    "    x = np.random.randint(low=1, high=10^5, size=(N, 1))\n",
    "    \n",
    "    x_sum_of_sqrs = np.square(x).sum()\n",
    "    x_sum = x.sum()\n",
    "    x_sqr_of_sum = (x_sum)^2\n",
    "    \n",
    "    diff = abs(x_sum_of_sqrs - (x_sqr_of_sum/(1.*N)))\n",
    "    return diff\n",
    "    \n",
    "diffs = [compute_diff() for t in range(T)]\n",
    "hist, edges = np.histogram(diffs, bins=10)\n",
    "print(hist)\n",
    "print(edges)\n",
    "plt.hist(hist)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms for computing variance\n",
    "==================\n",
    "[wiki](http://www.wikiwand.com/en/Algorithms_for_calculating_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data with offset 0: [ 4  7 13 16]\n",
      "method 1: sum-sqrs 490.00\n",
      "method 1: sqr-sum1  400.00\n",
      "method 1: sqr-sum1: 1600.00\n",
      "method 1: diff: 90.0\n",
      "method 1: variance: 30.00000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'variance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fa6f96b45211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\ndata with offset {1}: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mvariance_algo_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mvariance_algo_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-fa6f96b45211>\u001b[0m in \u001b[0;36mvariance_algo_2\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0msum2\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmean1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmean1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mvariance1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'method 2: variance: %.8f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mx_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'variance' is not defined"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# algo 1\n",
    "def variance_algo_1(x):\n",
    "    n, sum1, sum_sqrs = 0, 0, 0\n",
    "    for xx in x:\n",
    "        n += 1\n",
    "        sum1     += xx\n",
    "        sum_sqrs += xx*xx\n",
    "    print('method 1: sum-sqrs %.2f\\nmethod 1: sqr-sum1  %.2f' % (sum_sqrs, sum1*sum1/(1.*n)))\n",
    "    print('method 1: sqr-sum1: %.2f' % (sum1*sum1))\n",
    "    print('method 1: diff: {0}'.format((sum_sqrs - sum1*sum1/(1.*n))))\n",
    "    variance = (sum_sqrs - sum1*sum1/(1.*n))/(n-1.)\n",
    "    print('method 1: variance: %.8f' % (variance))\n",
    "\n",
    "# algo 2: 2-pass variance\n",
    "def variance_algo_2(x):\n",
    "    n, sum1, sum2=0, 0, 0\n",
    "    for xx in x:\n",
    "        n+=1\n",
    "        sum1 += xx\n",
    "    mean1 = sum1/(1.*n)\n",
    "    for xx in x:\n",
    "        sum2 += (xx-mean1)*(xx-mean1)\n",
    "    variance1 = sum2/(n - 1.)\n",
    "    print('method 2: variance: %.8f' % (variance))\n",
    "    \n",
    "x_original = [4,7,13,16]\n",
    "offsets = [0, 1e8, 1e9]\n",
    "for offset1 in offsets:\n",
    "    x = np.asarray([xx+offset1 for xx in x_original])\n",
    "    print('\\ndata with offset {1}: {0}'.format(x, offset1))\n",
    "    variance_algo_1(x)\n",
    "    variance_algo_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance & Correlation\n",
    "=============\n",
    "\n",
    "* Let X, Y be RV with means $\\mu_X \\text{ and } \\mu_Y$ and standard deviations $\\sigma_X \\text{ and } \\sigma_Y$. Then  \n",
    "$\\operatorname{Cov}(X, Y) = \\sigma(X, Y) = \\mathbb{E} \\left[ (X - \\mu_X) ~ (Y - \\mu_Y) \\right]$\n",
    "\n",
    "* Correlation $\\rho$ is defined as  $\\rho_{X,Y} = \\rho(X,Y) = \\frac{\\sigma(X, Y)}{\\sigma_X \\sigma_Y}$\n",
    "\n",
    "* **Alt-Forms**: $\\sigma(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X) \\mathbb{E}(Y)$\n",
    "\n",
    "* **Range**: $\\rho(X, Y) \\in [-1, 1]$\n",
    "\n",
    "* If Y = aX + b, then   \n",
    "$\n",
    "\\rho(X, Y) = \n",
    "\\begin{cases}\n",
    "    +1 & \\text{a > 0} \\\\\n",
    "    -1 & \\text{a < 0} \\\\\n",
    "     0 & \\text{X and Y are independent}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Expectation\n",
    "================\n",
    "\n",
    "**Definition**  \n",
    "The conditional expectation of X given Y=y is  \n",
    "$$\n",
    "\\mathbb{E}(X | Y=y) = \n",
    "\\begin{cases}\n",
    "    \\sum ~ x ~ f_{X | Y}(x|y) dx & \\text{discrete case}\\\\\n",
    "    \\int ~ x ~ f_{X | Y}(x|y) dx & \\text{continuous case}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Some more**\n",
    "* Let X take values from $\\mathcal{S}$ and Y from $\\mathcal{T} \\subseteq \\mathbb{R}$\n",
    "\n",
    "* **Regression function**  \n",
    "The function $v: S \\rightarrow \\mathbb{R}$, defined by $v(x) = \\mathbb{E}(Y | X=x)$ is called the *regression function* of Y based on X.\n",
    "\n",
    "* It should be note that $r(X) = \\mathbb{E}(Y|X)$ is not a value, but rather a function in Y.\n",
    "\n",
    "** Fundamental Property**\n",
    "1. $\\mathbb{E}[r(X) \\mathbb{E}(Y|X)] = \\mathbb{E}[r(X)Y]$ for every function $r: \\mathcal{S} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "**Properties**\n",
    "1. **Linearity**  \n",
    "  1. $\\mathbb{E}(Y+Z|X) = \\mathbb{E}(Y|X) + \\mathbb{E}(Z|X)$\n",
    "  1. $\\mathbb{E}(aY|X) = a\\mathbb{E}(Y|X) $\n",
    "1. **Monotonicity**  \n",
    "  1. if $Y \\ge 0, \\text{ then } \\mathbb{E}(Y|X) \\ge 0$\n",
    "  1. if $Y \\le Z, \\text{ then } \\mathbb{E}(Y|X) \\le \\mathbb{E}(Z|X)$\n",
    "  1. $| ~ \\mathbb{E}(Y|X) ~ | \\le \\mathbb{E}(|Y| ~ | X)$\n",
    "\n",
    "**Of Transformations of RVs**  \n",
    "If r(x,y) is a function of x and y, then  \n",
    "$$\n",
    "\\mathbb{E}(r(X,Y)| Y=y) = \n",
    "\\begin{cases}\n",
    "    \\sum ~ r(X,Y) ~ f_{X | Y}(x|y) dx & \\text{discrete case}\\\\\n",
    "    \\int ~ r(X,Y) ~ f_{X | Y}(x|y) dx & \\text{continuous case}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Iterated Expectations**  \n",
    "\n",
    "* **Equal means Property**   \n",
    "For RVs X and Y, if the expectations exist, then  0\n",
    "$\\mathbb{E} [ \\mathbb{E}(Y|X) ] = \\mathbb{E}(Y)$\n",
    "That is, the mean of Y and Y|X are the same.\n",
    "\n",
    "* For any function r(x,y), we have  \n",
    "$$\n",
    "\\mathbb{E} [ \\mathbb{E}(r(X,Y)|X) ] = \\mathbb{E}(r(X,Y))\n",
    "$$\n",
    "\n",
    "**Some results**\n",
    "\n",
    "* if $r: \\mathcal{S} \\rightarrow \\mathbb{R}, \\text{ then } Y - \\mathbb{E}(Y|X)$ and r(X) are uncorrelated.  \n",
    "Proof:  \n",
    "  * $\\mathbb{E}[Y - \\mathbb{E}(Y|X)] = \\mathbb{E}[Y] - \\mathbb{E}[\\mathbb{E}(Y|X)] = \\mathbb{E}[Y] - \\mathbb{E}[Y] = 0$\n",
    "  * $ \\sigma(Y−\\mathbb{E}(Y∣X),r(X)) \\\\\n",
    "      = \\mathbb{E}{[Y−E(Y∣X)]r(X)} - \\mathbb{E}[Y - \\mathbb{E}(Y|X)] \\mathbb{E}[r(X)]\\\\\n",
    "      =\\mathbb{E}[Yr(X)]−\\mathbb{E}[\\mathbb{E}(Y∣X)r(X)]\\\\\n",
    "      =0$\n",
    "\n",
    "* $If s:\\mathcal{S} \\rightarrow \\mathbb{R} \\text{ then } \\mathbb{E} [s(X) Y∣X] = s(X) E(Y∣X)$  \n",
    "That is, a deterministic functions acts like a constant.\n",
    "\n",
    "* **Substitution Rule**  \n",
    "if $s:\\mathcal{S} \\times \\mathcal{T} \\rightarrow \\mathbb{R}$, then\n",
    "$\\mathbb{E} [s(X, Y) | X=x] = \\mathbb{E} [s(x, Y) | X=x] $\n",
    "\n",
    "* if X, Y are independent, then $\\mathbb{E}(Y|X) = \\mathbb{E}(Y)$\n",
    "\n",
    "* **Consistency**  \n",
    "$\\mathbb{E} \\left[ \\mathbb{E}(Z~|~X,Y) ~|~ X \\right] = \\mathbb{E}(Z~|~X)$  \n",
    "$\\mathbb{E} \\left[ \\mathbb{E}(Z~|~X) ~|~ X,Y \\right] = \\mathbb{E}(Z~|~X)$\n",
    "\n",
    "* $\\sigma(X, \\mathbb{E}(Y ~|~ X)) = \\sigma(X, Y)$  \n",
    "Proof:  \n",
    "\\begin{array}{lllr}\n",
    "\\sigma(X, \\mathbb{E}(Y ~|~ X))\n",
    "&= \\mathbb{E}\\bigl[X ~ \\mathbb{E}(Y ~|~ X)\\bigr]\n",
    "& - \\mathbb{E}[X] ~ \\mathbb{E}\\bigl[\\mathbb{E}(Y ~|~ X))\\bigr] \\hspace{1pt}\n",
    "&  \\color{gray}{\\text{ deftn. of covariance}} \\\\\n",
    "&= \\mathbb{E}[XY]\n",
    "& - \\mathbb{E}[X] ~ \\mathbb{E}\\bigl[\\mathbb{E}(Y ~|~ X))\\bigr] \\hspace{1pt}\n",
    "&  \\color{gray}{\\text{ (fundamental property)}}\\\\\n",
    "&= \\mathbb{E}[XY]\n",
    "& - \\mathbb{E}[X] ~ \\mathbb{E}[Y]\n",
    "&  \\color{gray}{\\text{ (Equal means property)}} \\\\\n",
    "&= \\sigma(X, Y) & & \n",
    "\\end{array}\n",
    "\n",
    "\n",
    "* 1\n",
    "\n",
    "* todo: [consistency](http://www.math.uah.edu/stat/expect/Conditional.html#prp9) featuring [cond exp revisited](http://www.math.uah.edu/stat/expect/Conditional2.html),\n",
    "[cov and cor](http://www.math.uah.edu/stat/expect/Covariance.html#blp)\n",
    "\n",
    "\n",
    "**Conditional Variance**  \n",
    "$\\mathbb{V}(Y | X=x) = \\int (y - \\mu(x))^2 ~ f(y|x) ~ dx$  \n",
    "where $\\mu(x) = \\mathbb{E}(Y | X=x)$\n",
    "\n",
    "**Variance**  \n",
    "$\\mathbb{V}(Y) = \\mathbb{E}\\mathbb{V}(Y|X) + \\mathbb{V}\\mathbb{E}(Y|X)$  \n",
    "That is, Variance of Y is the sum of expected variance and the variance of expected conditional.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import notebook\n",
    "E = notebook.nbextensions.EnableNBExtensionApp()\n",
    "E.enable_nbextension('calico-document-tools')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--bibtex\n",
    "\n",
    "@Article{PER-GRA:2007,\n",
    "  Author    = {P\\'erez, Fernando and Granger, Brian E.},\n",
    "  Title     = {{IP}ython: a System for Interactive Scientific Computing},\n",
    "  Journal   = {Computing in Science and Engineering},\n",
    "  Volume    = {9},\n",
    "  Number    = {3},\n",
    "  Pages     = {21--29},\n",
    "  month     = may,\n",
    "  year      = 2007,\n",
    "  url       = \"http://ipython.org\",\n",
    "  ISSN      = \"1521-9615\",\n",
    "  doi       = {10.1109/MCSE.2007.53},\n",
    "  publisher = {IEEE Computer Society},\n",
    "}\n",
    "\n",
    "@article{Papa2007,\n",
    "  author = {Papa, David A. and Markov, Igor L.},\n",
    "  journal = {Approximation algorithms and metaheuristics},\n",
    "  pages = {1--38},\n",
    "  title = {{Hypergraph partitioning and clustering}},\n",
    "  url = {http://www.podload.org/pubs/book/part\\_survey.pdf},\n",
    "  year = {2007}\n",
    "}\n",
    "\n",
    "-->\n",
    "\n",
    "Examples of citations: [CITE](#cite-PER-GRA:2007) or [CITE](#cite-Papa2007)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
