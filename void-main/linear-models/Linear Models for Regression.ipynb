{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "$ \\newcommand{\\E}[1]{\\mathbb{E}\\left[#1\\right]}$  \n",
    "$ \\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}$\n",
    "$ \\newcommand{\\EXP}[1]{\\exp\\left(#1\\right)}$  \n",
    "$ \\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\n",
    "\\newcommand{\\sumnN}{\\sum_{n=1}^{N}}\n",
    "\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr}\n",
    "#1\n",
    "\\end{array}\n",
    "}\n",
    "\\newcommand{\\mat}[1]{\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "#1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\dmc}{\\mathcal{D}}\n",
    "\\newcommand{\\norm}[1]{\\|#1\\|}\n",
    "\\newcommand{\\normsqr}[1]{\\norm{#1}^2}\n",
    "\\newcommand{\\frachalf}[1]{\\frac{#1}{2}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\Abt}{\\Ab^T}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\Ib}{\\mathbf{I}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\Sb}{\\mathbf{S}}\n",
    "\\newcommand{\\tb}{\\mathbf{t}}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\wb}{\\mathbf{w}}\n",
    "\\newcommand{\\wbt}{\\wb^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\Phib}{\\pmb{\\Phi}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\hx}{h(\\xb)}\n",
    "\\newcommand{\\yx}{y(\\xb; \\mathcal{D})}\n",
    "\\newcommand{\\ed}[1]{\\mathbb{E}_D\\left[ #1 \\right]}\n",
    "\\newcommand{\\edyx}{\\ed{\\yx}}\n",
    "\\newcommand{\\px}{~p(\\xb)}\n",
    "\\newcommand{\\dx}{~d\\xb}\n",
    "\\newcommand{\\pxdx}{\\px \\dx}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**  \n",
    "1. Use different basis models for the same problem\n",
    "1. Compare w/ and w/o regularization\n",
    "1. Use different regularizers for the same problem\n",
    "1. Bias Variance Decomposition + Experiments\n",
    "\n",
    "**Questions**\n",
    "1. Equation \\ref{eq:ptw} under ML and Least Squares\n",
    "1. Why does Lasso act as a feature selector?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "=======\n",
    "\n",
    "Simple Linear Model\n",
    "------------------\n",
    "\n",
    "y(\\textbf{x},**w**) $ = w_0 + \\sum_{i=1}^{D} w_i x_i$ \n",
    "\n",
    "where  \n",
    "**x** $ = (x_1, \\cdots, x_D)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Functions\n",
    "---------------\n",
    "\n",
    "$\n",
    "y(\\mathbf{x},\\mathbf{w}) = w_0 + \\sum_{j=1}^{M-1} w_i ~ \\phi_j(\\mathbf{x})\n",
    "$ \n",
    "\n",
    "* where $\\phi_j(\\mathbf{x})$ are called basis functions. \n",
    "* Total #parameters = M\n",
    "\n",
    "If $\\phi_0(\\mathbf{x}) = 1$, then  \n",
    "$y(\\mathbf{x},\\mathbf{w})  = \\sum_{j=0}^{M-1} w_i ~ \\phi_j(\\text{x}) = \\mathbf{w}^T \\phi(\\mathbf{x})$ \n",
    "\n",
    "* Linear in **w**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choices of Basis Functions\n",
    "-------------------------\n",
    "\n",
    "1. Polynomial Basis\n",
    "  * $\\phi_j(\\mathbf{x}) = x^j$\n",
    "  * Limitation: Global models\n",
    "  * Cure: Spline Functions: fit different polynomials based on region [EOSL Hastie]\n",
    "1. Gaussian Basis FUnction:\n",
    "  * $\\phi_j(\\mathbf{x}) = \\EXP{-\\frac{(x-\\mu_j)^2}{2s^2}}$\n",
    "  * Need not be a pdf\n",
    "1. Sigmoidal\n",
    "  * $\\phi_j(\\mathbf{x}) = \\sigma \\left( \\frac{x-\\mu_j}{s} \\right)$\n",
    "  * where $\\sigma(a) = \\frac{1}{1+\\EXP{-a}}$ is the logistic sigmoid function\n",
    "  * Can use $\\tanh$, since $\\tanh(a) = 2\\sigma(2a) - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions for Regression\n",
    "=================\n",
    "\n",
    "[PRML]1.5.5\n",
    "\n",
    "Say we fit a function y(x) to give the target variable t. \n",
    "Let $\\mathcal{L}(t, y(x))$ denote the loss function.\n",
    "\n",
    "Then, Average or Expected Loss is given by,\n",
    "$\\E{\\mathcal{L}} = \\iint \\mathcal{L}(t, y(x)) ~p(x,t) ~dx ~dt$\n",
    "\n",
    "If we consider the squared loss function $\\mathcal{L}(t, y(\\mathbf{x})) = \\left( t - y(\\mathbf{x})\\right)^2$, then,\n",
    "\n",
    "\\begin{array}{llr}\n",
    "\\E{\\mathcal{L}} \n",
    "&= \\iint \\left( t - y(\\mathbf{x})\\right)^2 ~p(\\mathbf{x},t) ~d\\mathbf{x} ~dt\n",
    "\\\\\n",
    "\\dfrac{\\partial \\E{\\mathcal{L}}}{\\partial y(\\mathbf{x})}\n",
    "&=\n",
    "2 \\int \\left( t - y(\\mathbf{x})\\right) ~p(\\mathbf{x},t) ~dt = 0\n",
    "\\\\\n",
    "\\int t ~p(\\mathbf{x},t) ~dt  \n",
    "&=\n",
    "\\int y(\\mathbf{x}) ~p(\\mathbf{x},t) ~dt = y(\\mathbf{x}) \\int  ~p(\\mathbf{x},t) ~dt\n",
    "\\\\\n",
    "\\int t ~p(\\mathbf{x},t) ~dt  \n",
    "&=\n",
    "y(\\mathbf{x}) p(\\mathbf{x})\n",
    "\\\\\n",
    "y(\\mathbf{x})\n",
    "&= \n",
    "\\dfrac{\\int t ~p(\\mathbf{x},t) ~dt  }{p(\\mathbf{x})}\n",
    "= \\int t ~p(t \\mid \\mathbf{x}) ~dt\n",
    "\\\\\n",
    "y(\\mathbf{x}) \n",
    "&= \\mathbb{E}_t \\left[ t \\mid \\mathbf{x} \\right]\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\color{green}{\n",
    "  \\text{ Thus for a squared loss function, the optimal prediction \n",
    "  is given by the conditional mean of the target variable}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt. Derivation   \n",
    "\n",
    "$t = \\E{\\E{t \\mid \\mathbf{x}}}$\n",
    "hence, $\\E{\\E{t \\mid \\mathbf{x}} - t}^2 = \\V{t \\mid \\mathbf{x}}$ \n",
    "\n",
    "\\begin{array}{llr}\n",
    "\\left( y(\\mathbf{x}) - t \\right)^2\n",
    "&=\n",
    "\\left( y(\\mathbf{x}) - \\E{t \\mid X} + \\E{t \\mid \\mathbf{x}} - t \\right)^2\n",
    "\\\\\n",
    "&=\n",
    "\\left(\n",
    "    y(\\mathbf{x}) - \\E{t \\mid \\mathbf{x}} \n",
    "\\right)^2 \n",
    "+\n",
    "2\n",
    "\\left(\n",
    "    y(\\mathbf{x}) - \\E{t \\mid \\mathbf{x}}\n",
    "\\right)\n",
    "\\left(\n",
    "    \\E{t \\mid \\mathbf{x}} - t\n",
    "\\right)\n",
    "+\n",
    "\\left(\n",
    "    \\E{t \\mid \\mathbf{x}} - t\n",
    "\\right)^2\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting into $\\E{\\mathcal{L}}$, the cross term vanishes (the first term of the cross term). Hence  \n",
    "$$\n",
    "\\E{\\mathcal{L}} =\n",
    "\\underbrace{\\int\n",
    "\\left(\n",
    "    y(\\mathbf{x}) - \\E{t \\mid \\mathbf{x}} \n",
    "\\right)^2 ~p(\\mathbf{x}) ~d\\mathbf{x}\n",
    "}_{\\text{Bias Term}}\n",
    "-\n",
    "\\underbrace{\n",
    "\\int\n",
    "\\V{t \\mid \\mathbf{x}} ~p(\\mathbf{x}) ~d\\mathbf{x}\n",
    "}_{\\text{Variance term}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML and Least Squares\n",
    "===========\n",
    "\n",
    "Let *t* be given by a deterministic *y* and additive Gaussian noise as,\n",
    "$t = y(\\mathbf{x},\\mathbf{w}) + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$  \n",
    "Then,  \n",
    "<div id='GaussianIidLikelihood'/>\n",
    "$$\n",
    "p(t \\mid \\xb, \\wb, \\beta) = \\mathcal{N}(t \\mid y(\\xb,\\wb), \\beta^{-1})\n",
    "=\n",
    "\\left( \\dfrac{\\beta}{2\\pi} \\right)^{1/2}\n",
    "\\exp\n",
    "\\left\\{\n",
    "-\\dfrac{\\beta}{2} (t - \\wbt \\phi(\\xb))^2\n",
    "\\right\\}\n",
    "\\label{eq:ptw}\n",
    "$$\n",
    "[?]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a squared loss function, optimal t is given by conditional mean of t. In case of a Gaussian, it becomes\n",
    "$\\E{t \\mid \\mathbf{x}} = \\int t ~p(t \\mid \\mathbf{x}) ~dt = y(\\mathbf{x},\\mathbf{w})$\n",
    "Refer: <a href=\"#Loss-Functions-for-Regression\">Loss Functions for Regression</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $\\mathbf{x}$ is IID Normal given by $\\ref{eq:ptw}$, then  \n",
    "$$\n",
    "p(\\mathbf{t} \\mid \\mathbf{X}, \\mathbf{w}, \\beta)\n",
    "=\n",
    "\\prod_{n=1}^{N}\n",
    "\\mathcal{N}\n",
    "    \\left(\n",
    "        t_n \\mid \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x_n}), \\beta^{-1}\n",
    "    \\right)\n",
    "\\label{eq:ptiid}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the log likehood, we get  \n",
    "\\begin{array}{ll}\n",
    "\\ln p(\\mathbf{t} \\mid \\mathbf{w}, \\beta)\n",
    "&=\n",
    "\\frac{N}{2} \\ln \\beta\n",
    "-\n",
    "\\frac{N}{2} \\ln(2\\pi)\n",
    "-\n",
    "\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Estimation\n",
    "-------------------\n",
    "Grad of the log likehood (wrt $\\mathbf{w}$) gives,  \n",
    "\\begin{array}{ll}\n",
    "\\nabla \\ln p(\\mathbf{t} | \\mathbf{w}, \\beta)\n",
    "&=\n",
    "\\beta\n",
    "\\sum_{n=1}^{N}\n",
    "(t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "= 0\n",
    "\\\\\n",
    "\\sum_{n=1}^{N}\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "-\n",
    "\\mathbf{w}^T \n",
    "\\sum_{n=1}^{N}\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "&= 0\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\phi}\n",
    "=\n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "        \\phi_1\\\\\n",
    "        \\phi_2\\\\\n",
    "        \\vdots\\\\\n",
    "        \\phi_{M-1}\n",
    "    \\end{matrix}\n",
    "\\right]\n",
    "\\hspace{5pt}\n",
    "\\mathbf{t}\n",
    "=\n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "        t_1\\\\\n",
    "        t_2\\\\\n",
    "        \\vdots\\\\\n",
    "        t_{N}\n",
    "    \\end{matrix}\n",
    "\\right]\n",
    "\\hspace{20pt}\n",
    "\\mathbf{\\phi} \\mathbf{\\phi}^T\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\phi_0 \\phi_0 & \\phi_0 \\phi_1 & \\cdots & \\phi_0 \\phi_{M-1}\\\\\n",
    "\\phi_1 \\phi_0 & \\phi_1 \\phi_1 & \\cdots & \\phi_1 \\phi_{M-1}\\\\\n",
    "\\vdots        & \\vdots        & \\ddots       & \\vdots\\\\\n",
    "\\phi_{M-1} \\phi_0 & \\phi_{M-1} \\phi_1 & \\cdots & \\phi_{M-1} \\phi_{M-1}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we let\n",
    "$$\n",
    "\\Phi(\\mathbf{X})\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\phi_0(x_1) & \\phi_1(x_1) & \\cdots & \\phi_{M-1}(x_1)\\\\\n",
    "\\phi_0(x_2) & \\phi_1(x_2) & \\cdots & \\phi_{M-1}(x_2)\\\\\n",
    "\\vdots      & \\vdots      & \\ddots & \\vdots\\\\\n",
    "\\phi_0(x_N) & \\phi_1(x_N) & \\cdots & \\phi_{M-1}(x_N)\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N}\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) ~ \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "=\n",
    "\\Phi(\\mathbf{X})^T \\Phi(\\mathbf{X})\n",
    "\\\\\n",
    "\\sum_{n=1}^{N}\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "=\n",
    "\\Phi(\\mathbf{X})^T \\mathbf{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$\n",
    "\\mathbf{w}_{ML} = \\left(\\Phi^T \\Phi \\right)^{-1} \\Phi^T \\mathbf{t}\n",
    "$\n",
    "which is called *normal equations* for least squares.  \n",
    "$\\Phi$ is called the *design matrix* which is $N \\times M$ matrix.  \n",
    "Each row of $\\Phi$ is a feature vector transposed $\\phi(x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Parameter\n",
    "---------------\n",
    "\n",
    "Now, consider the last term and seperate out the bias parameter\n",
    "$\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - w_0 - \\sum_{m=1}^{M-1} w_m \\phi_m(\\mathbf{x}_n)\n",
    "\\right)^2$  \n",
    "Diff wrt $w_0$ and equating it to zero, we get  \n",
    "\n",
    "\\begin{array}{ll}\n",
    "0\n",
    "&=\n",
    "\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "2(t_n - w_0 - \\sum_{m=1}^{M-1} w_m \\phi_m(\\mathbf{x}_n))\n",
    "\\\\\n",
    "N w_0\n",
    "&=\n",
    "\\sum_{n=1}^{N}\n",
    "t_n\n",
    "-\n",
    "\\sum_{n=1}^{N}\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m \\phi_m(\\mathbf{x}_n)\n",
    "\\\\\n",
    "N w_0\n",
    "&=\n",
    "\\sum_{n=1}^{N}\n",
    "t_n\n",
    "-\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m \n",
    "\\sum_{n=1}^{N}\n",
    "\\phi_m(\\mathbf{x}_n)\n",
    "\\\\\n",
    "w_0\n",
    "&=\n",
    "\\overline{t}_n\n",
    "-\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m\n",
    "\\overline{\\phi}_m\n",
    "\\\\\n",
    "\\text{where}\n",
    "\\\\\n",
    "\\overline{t}_n &= \\frac{1}{N} \\sum_{n=1}^{N} t_n \\\\\n",
    "\\overline{\\phi}_m &= \\frac{1}{N} \\sum_{n=1}^{N} \\phi_m(\\mathbf{x}_n) \\\\\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "\n",
    "Thus the basis $w_0$ compensates for the difference between the average of target values\n",
    "and\n",
    "the weighted sum of the average of the basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameter\n",
    "--------------\n",
    "\n",
    "Diff log likelihood wrt $\\beta$ and equating it to zero, we get  \n",
    "\\begin{array}{ll}\n",
    "0\n",
    "&=\n",
    "\\frac{N}{2}\n",
    "\\frac{1}{\\beta}\n",
    "-\n",
    "\\frac{1}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\\\\\n",
    "\\frac{1}{\\beta_{ML}}\n",
    "&=\n",
    "\\frac{1}{N}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\\\\\n",
    "\\end{array}\n",
    "That is, the inverse of precision is the variance of the target\n",
    "about the regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Learning\n",
    "===========\n",
    "\n",
    "* Stochastic Gradient can be used to find the parameters sequentially.\n",
    "* Update rule: $\\mathbf{w}_{\\tau+1} = \\mathbf{w}_{\\tau} + \\eta \\nabla E(\\mathbf{w})$\n",
    "* If squared loss function is assumed,  \n",
    "  $\\nabla E_D(\\mathbf{w}) = \n",
    "  \\left( \n",
    "      t_n - \n",
    "      \\mathbf{w}_{\\tau}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "  \\right)\n",
    "  \\mathbf{\\phi}(\\mathbf{x}_n)$\n",
    "* Hence, the update rule becomes,  \n",
    "  $\\mathbf{w}_{\\tau+1}\n",
    "  = \\mathbf{w}_{\\tau}\n",
    "    + \\eta\n",
    "      \\left( \n",
    "          t_n - \n",
    "          \\mathbf{w}_{\\tau}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "      \\right)\n",
    "      \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "========\n",
    "\n",
    "* Form: $E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Regularization\n",
    "-----------------------\n",
    "\n",
    "* $E_W(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^T \\mathbf{w}$\n",
    "* Called as *weight decay* since $\\mathbf{w}$ decays to zero when $\\lambda$ is high\n",
    "* Called as *Parameter Shrinkage* as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total Error Function:  \n",
    "\\begin{array}{rlr}\n",
    "E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w})\n",
    "&=\n",
    "\\frac{1}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "(t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "+\n",
    "\\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}\n",
    "\\\\\n",
    "0\n",
    "&=\n",
    "\\sum_{n=1}^{N}\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "-\n",
    "\\mathbf{w}^T \n",
    "\\sum_{n=1}^{N}\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "+ \\lambda \\mathbf{w}^T\n",
    "& \\color{gray}{\\text{Diff wrt w}}\n",
    "\\\\\n",
    "0\n",
    "&=\n",
    "\\mathbf{t}^T \\Phi - \\mathbf{w}^T \\Phi^T \\Phi + \\lambda \\mathbf{w}^T\n",
    "\\\\\n",
    "\\left( \\Phi^T \\Phi - \\lambda \\mathcal{I} \\right) \\mathbf{w} \n",
    "&=\n",
    "\\Phi^T \\mathbf{t}\n",
    "\\\\\n",
    "\\mathbf{w}\n",
    "&= \n",
    "\\left( \n",
    "    \\Phi^T \\Phi - \\lambda \\mathcal{I}\n",
    "\\right)^{-1} \n",
    "\\Phi^T \\mathbf{t}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalized Regularizer\n",
    "---------------------\n",
    "* $\\frac{\\lambda}{2} \\sum_{m=0}^{M-1} \\left| \\mathbf{w_j} \\right|^q$\n",
    "* Allows complex models to be fit on smaller data sets\n",
    "* The problem of finding the right model complexity transforms to that of finding the right value for $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3eeeed43a71c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mthetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthetas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def sign(x):\n",
    "    return +1 if x>=0 else -1\n",
    "\n",
    "thetas = np.linspace(0,2*math.pi,10**3)\n",
    "pts = np.asarray([[math.cos(theta),math.sin(theta)] for theta in thetas])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in [abs(pts[ix,0]-pts[100-1-ix,0])<1e-5 for ix in range(50)] if x is False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b215e37fd770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteract_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'interact' is not defined"
     ]
    }
   ],
   "source": [
    "# q=1/2\n",
    "def show_regularizer(q=0.5):\n",
    "    ptsq = np.zeros_like(pts)\n",
    "    for ix in range(math.floor(ptsq.shape[0]/2.)):\n",
    "        x = pts[ix,0]\n",
    "        ptsq[ix,0] = ptsq[100-1-ix,0] = x\n",
    "        y_abs = abs((1 - abs(x)**q)**(1./q))\n",
    "        ptsq[ix,1] = y_abs\n",
    "        ptsq[100-1-ix,1] = -y_abs\n",
    "    plt.plot(ptsq[:,0],ptsq[:,1],'.',label='q='+str(q))\n",
    "    \n",
    "def interact_regularizer(q=0.5,offset=0.7,show_all=True):\n",
    "    qs = [0.5,1.,2.,4.] if show_all else [q]\n",
    "    [show_regularizer(qq) for qq in qs]\n",
    "    plt.xlim(plt.xlim()[0]-0.1,plt.xlim()[1]+offset)\n",
    "    plt.ylim(plt.ylim()[0]-0.1,plt.ylim()[1]+0.1)\n",
    "    plt.legend()\n",
    "    plt.title('Contours of Reqularizion function')\n",
    "    plt.show()\n",
    "    \n",
    "interact(interact_regularizer,q=(.5,4,.5),offset=(0,1.,.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso\n",
    "-----\n",
    "\n",
    "* By setting $q=1$ in the generalized regularizer\n",
    "* Has a tendency to set some weights to zero, thereby making it a \"feature selector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Outputs\n",
    "==========\n",
    "\n",
    "* Let\n",
    "  * $\\mathbf{y}(\\mathbf{x},\\mathbf{w})$ be K dimensional\n",
    "  * W be $M \\times K$ matrix of parameters\n",
    "  * $\\mathbf{\\phi}(\\mathbf{x})$ is M dimensional\n",
    "  * $\\mathbf{t}$ is K dimensional\n",
    "\n",
    "\\begin{array}{rlr}\n",
    "p(\\mathbf{t} \\mid \\mathbf{x}, \\mathbf{W}, \\beta)\n",
    "&=\n",
    "\\mathcal{N}\n",
    "\\left(\n",
    "\\mathbf{t} \\mid \\mathbf{W}^T \\mathbf{\\phi}(\\mathbf{x}),\n",
    "\\beta^{-1} \\mathcal{I} \n",
    "\\right)\n",
    "\\\\\n",
    "\\ln p = 0\n",
    "&=\n",
    "\\frac{NK}{2} \\ln\\left(\\frac{\\beta}{2\\pi}\\right)\n",
    "-\n",
    "\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left\\|\n",
    "\\mathbf{t}_n\n",
    "-\n",
    "\\mathbf{W}^T \\mathbf{\\phi}(\\mathbf{x})\n",
    "\\right\\|^2\n",
    "&\n",
    "\\color{gray}{\\text{See: Multivariate Gaussian}}\n",
    "\\\\\n",
    "\\mathbf{W}_{ML}\n",
    "&=\n",
    "\\left(\n",
    "\\Phi^T \\Phi\n",
    "\\right)^{-1}\n",
    "\\Phi^T \\mathbf{T}\n",
    "= \n",
    "\\Phi^{\\dagger} \\mathbf{T}\n",
    "\\end{array}\n",
    "* That is, the same $\\Phi^{\\dagger}$ can be used for all K output target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Variance Decomposition\n",
    "===============\n",
    "\n",
    "* Frequentist viewpoint of the model complexity issue\n",
    "* For a squared loss function, the optimal prediction is given by the conditional expectation h(x),  \n",
    "  $$h(\\xb) = \\E{t \\mid \\xb} = \\int t ~p (t \\mid \\xb) ~dt$$\n",
    "* The expected square loss can be written as,\n",
    "$$\n",
    "\\E{\\mathcal{L}}\n",
    "=\n",
    "\\int \\left(\n",
    "    y(\\xb) - h(\\xb)\n",
    "\\right)^2\n",
    "~p(\\xb) ~d\\xb\n",
    "+\n",
    "\\iint \\left(\n",
    "    h(\\xb) - t\n",
    "\\right)^2\n",
    "~p(\\xb, t) ~dx dt\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the First term $\\left(y(\\mathbf{x}) - h(\\mathbf{x})\\right)^2$.  \n",
    "$\\pm \\edyx$, we get  \n",
    "$$\n",
    "\\left\\{\\yx - \\edyx + \\edyx - \\hx\\right\\}^2\n",
    "\\\\\n",
    "\\hspace{10pt}=\n",
    "\\left(\n",
    "    \\yx - \\edyx\n",
    "\\right)^2\n",
    "+\n",
    "\\left(\n",
    "    \\edyx - \\hx\n",
    "\\right)^2\n",
    "+\n",
    "2\n",
    "\\left(\\yx - \\edyx\\right)\n",
    "\\left(\\edyx - \\hx\\right)\n",
    "$$\n",
    "\n",
    "Taking the expectation wrt $\\mathcal{D}$, the last term vanishes.  \n",
    "$$\n",
    "\\ed{\\left(\\yx - \\hx\\right)^2}\n",
    "=\n",
    "\\underbrace{\\left(\n",
    "    \\ed{\\yx - \\hx}\n",
    "\\right)^2}_{(\\text{bias})^2}\n",
    "+\n",
    "\\underbrace{\n",
    "\\ed{\n",
    " \\left(\n",
    "     \\yx - \\edyx\n",
    " \\right)\n",
    "}}_{\\text{variance}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Squared-bias: Deviation of the average prediction from the desired\n",
    "1. Variation of the individual predictions about the average prediction\n",
    "\n",
    "Thus,  \n",
    "\\begin{array}{ll}\n",
    "\\text{expected loss} &= (\\text{bias})^2 + \\text{variance} + \\text{noise}\\\\\n",
    "&\\color{green}{\\text{where}}&\\\\\n",
    "(\\text{bias})^2\n",
    "&=\n",
    "\\int \\left(\\edyx - \\hx\\right)^2 \\pxdx\n",
    "\\\\\n",
    "\\text{variance}\n",
    "&=\n",
    "\\int \\ed{\\left( \\yx - \\edyx \\right)^2} \\pxdx\n",
    "\\\\\n",
    "\\text{noise}\n",
    "&=\n",
    "\\iint \\left(\\hx - t\\right)^2 ~p(\\xb,t) \\dx ~dt\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Value**  \n",
    "* Zilch\n",
    "* we have only a single data set\n",
    "* If there are large no. of data sets, we can combine them into a single large dataset\n",
    "* This would reduce the overall level of overfitting for a given model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Linear Regression\n",
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter distribution\n",
    "-----------------------\n",
    "\n",
    "From <a href='#GaussianIidLikelihood'>Likelihood</a> given by\n",
    "$$\n",
    "p(\\tb \\mid \\wb) =\n",
    "\\left(\n",
    "  \\dfrac{\\beta}{2\\pi}\n",
    "\\right)^{N/2}\n",
    "\\exp\n",
    "\\left\\{\n",
    "  -\\dfrac{\\beta}{2}\n",
    "  \\sumnN \\left( t_n - \\wbt \\phi\\left(\\xb_n\\right) \\right)^2\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "Since the likelihood is exponential of quadratic function of **w**,\n",
    "the prior is a Gaussian given by\n",
    "$$\n",
    "p(\\wb)\n",
    "=\n",
    "\\Nl{\\wb}{\\mb_0}{\\Sb_0}\n",
    "$$\n",
    "Now we need to find $p(\\wb \\mid \\tb)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few things to remember (<a href='../Gaussian Stuff.ipynb#BayesTheoremForGaussianVariables'>Bayes' Theorem for Gaussian Variables</a>)\n",
    "\n",
    "Given a marginal Gaussian for **x** and a conditional gaussian for **y** given **x** of the form\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\xb) \n",
    "&=\n",
    "\\Nl{\\xb}{\\mub}{\\li}\n",
    "\\\\\n",
    "p(\\yb \\mid \\xb)\n",
    "&=\n",
    "\\Nl{\\yb}{\\Ab\\xb + \\bb}{\\Lbi}\n",
    "\\\\\n",
    "\\color{green}{\\text{Marginal }}\n",
    "p(\\yb)\n",
    "&=\n",
    "\\Nl{\\yb}{\\Ab\\mub+\\bb}{\\Lbi + \\Ab\\Lambda^{-1}\\Abt}\n",
    "\\\\\n",
    "\\color{green}{\\text{Conditional }}\n",
    "p(\\xb \\mid \\yb)\n",
    "&=\n",
    "\\Nl\n",
    "{\\xb}\n",
    "{\\Sigma \\left\\{ \\Abt\\Lb(\\yb-\\bb) + \\Lambda\\mub \\right\\}}\n",
    "{\\Sigma}\n",
    "\\\\\n",
    "\\text{where}\n",
    "\\\\\n",
    "\\Sigma\n",
    "&= \\left(\\Lambda + \\Abt\\Lb\\Ab\\right)^{-1}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus\n",
    "$$\n",
    "\\arrthree{\n",
    "\\mub  &\\equiv \\mb_0 \\\\\n",
    "\\li &\\equiv \\Sb_0 \\\\\n",
    "\\Ab &\\equiv \\Phi \\\\\n",
    "\\yb &\\equiv \\tb \\\\\n",
    "\\Lb &\\equiv \\beta\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the posterior is given by\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\wb \\mid \\tb)\n",
    "&=\n",
    "\\Nl{\\wb}{\\mb_N}{\\Sb_N}\n",
    "\\\\\n",
    "\\text{where}\n",
    "\\\\\n",
    "\\mb_N\n",
    "&=\n",
    "\\Sigma \\left\\{ \\Abt\\Lb(\\yb-\\bb) + \\Lambda\\mub \\right\\}\n",
    "&=\n",
    "\\Sb_N \\left\\{ \\Phi^T \\beta \\tb + \\Sb_0^{-1} \\mb_0\\right\\}\n",
    "\\\\\n",
    "&=\n",
    "\\Sb_N \\left( \\beta \\Phi^T \\tb + \\Sb_0^{-1} \\mb_0\\right)\n",
    "\\\\\n",
    "\\Sb_N^{-1}\n",
    "&=\n",
    "\\left(\\Lambda + \\Abt\\Lb\\Ab\\right)\n",
    "&=\n",
    "\\left(\\Sb_0^{-1} + \\Phi^T \\beta \\Phi\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\left(\\Sb_0^{-1} + \\beta \\Phi^T \\Phi\\right)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider the prior as an isotropic Gaussian with precision $\\alpha$, such that\n",
    "<div id='BayesianLinearRegressionPosteriorParameters'/>\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\wb \\mid \\alpha)\n",
    "&=\n",
    "\\Nl{\\wb}{\\mathbf{0}}{\\alpha^{-1}\\Ib}\n",
    "\\\\\n",
    "\\text{Thus the posterior} & \\text{ parameters becomes}\n",
    "\\\\\n",
    "\\mb_N\n",
    "&=\n",
    "\\Sb_N (\\beta \\Phib^T \\tb)\n",
    "=\n",
    "\\beta \\Sb_N \\Phib^T \\tb\n",
    "\\\\\n",
    "\\Sb_N^{-1} \n",
    "&=\n",
    "\\alpha\\Ib + \\beta \\Phib^T \\Phib\n",
    "}\n",
    "$$\n",
    "The log of the posterior given by the sum of the log likelihood and log prior, as\n",
    "$$\n",
    "\\ln p(\\wb \\mid \\tb)\n",
    "=\n",
    "-\\dfrac{\\beta}{2}\n",
    "\\sumnN \\left\\{ t_n - \\wbt \\phi(\\xb_n)\\right\\}\n",
    "-\\dfrac{\\alpha}{2} \\wbt \\wb\n",
    "+ \\text{ const}\n",
    "$$\n",
    "The max of this posterior wrt **w** is equivalent to \n",
    "minimization of the sum-of-the-squares error function\n",
    "with the addition of the quadratic regularization term,\n",
    "with $\\lambda = \\alpha / \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.31317987]\n",
      " [ 0.52563142]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAREAAAEACAYAAACUHkKwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIkEUIiISRCEiIkEUIiISRCEiIkH+H9Zqta+ins9TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda4eef4e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interact_bay_lin_reg>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "def fun_f(x, a):\n",
    "    return a[0] + a[1]*1.*x\n",
    "\n",
    "def show_gaussian(mean, precision_mat):\n",
    "    in_pts = 10**2\n",
    "    x = np.linspace(-1,1,in_pts)\n",
    "    xv,yv=np.meshgrid(x,x)\n",
    "    #(2 \\pi \\beta)^(-1/2)\n",
    "    coeff =math.sqrt(np.linalg.det(precision_mat)/(2.*math.pi))\n",
    "    img = np.zeros((in_pts,in_pts))\n",
    "    for ix in range(in_pts):\n",
    "        for iy in range(in_pts):\n",
    "            x = np.matrix([xv[ix,iy],yv[ix,iy]]).transpose()\n",
    "            # -1/2 * (x-mu)T Precision (x-mu)\n",
    "            expt = -1./2. * (x - mean).transpose() * precision_mat * (x-mean)\n",
    "            img[ix,iy] = coeff * math.exp(expt[0,0])\n",
    "    \n",
    "    ticks = np.arange(-1,1,1.0)\n",
    "    extent = (-1,1,-1,1)\n",
    "    plt.imshow(img, extent=extent, origin='lower')\n",
    "\n",
    "def show_bay_lin_reg(in_samples,a0,a1,beta_sd,alpha):\n",
    "    mean_0 = np.matrix([0,0]).transpose()\n",
    "    xn = np.random.rand(in_samples)*2.0 - 1.0\n",
    "    fn = fun_f(xn, [a0,a1])\n",
    "    xn_noise = np.random.normal(0,beta_sd,in_samples)\n",
    "    # tn = a0 + a1*x + N(0,1/beta)\n",
    "    tn = np.matrix(fn + xn_noise).reshape(in_samples,1)\n",
    "    # phi = [ones^T x^T]\n",
    "    phi = np.matrix(np.vstack([np.ones_like(xn),xn])).transpose()\n",
    "    # Sn = beta*phi^T*phi + alpha*I\n",
    "    sn_inv = beta * phi.transpose() * phi + alpha*np.eye(2)\n",
    "    sn = np.linalg.inv(sn_inv)\n",
    "    # m_n = beta*Sn*Phi^T*Tn\n",
    "    mean_n = beta * sn * phi.transpose() * tn\n",
    "    print(mean_n)\n",
    "    show_gaussian(mean_n, sn_inv)\n",
    "    plt.plot(a0,a1,'+w',markersize=30)\n",
    "    plt.show()\n",
    "def interact_bay_lin_reg(in_samples=40,\n",
    "                         a0=-0.3,a1=0.5,\n",
    "                         beta_sd=0.2,alpha=2.0,\n",
    "                         show_all=True):\n",
    "    show_bay_lin_reg(in_samples,a0,a1,beta_sd,alpha)\n",
    "    \n",
    "interact(interact_bay_lin_reg,\n",
    "         in_samples=(1,200),\n",
    "         a0=(-1,1,0.1),a1=(-1,1,0.1),\n",
    "         beta_sd=(0,1,0.05),alpha=(1,5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive Distribution\n",
    "-----------------------\n",
    "\n",
    "* Usually not interested in the value of **w** itself.\n",
    "* need to make prediction of *t* for new values of **x**\n",
    "* Predictive Distribution $\n",
    "p(t \\mid \\tb, \\alpha, \\beta)\n",
    "=\n",
    "\\int p(t \\mid \\wb, \\beta) p(\\wb \\mid \\tb, \\alpha, \\beta)\n",
    "$\n",
    "* The conditional distribution is given by\n",
    "$$\n",
    "p(t \\mid \\wb, \\beta) = \\Nl{t}{y(\\xb,\\wb)}{\\beta^{-1}}\n",
    "$$\n",
    "* Posterior is given by\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\wb \\mid \\tb)\n",
    "&=\n",
    "\\Nl{\\wb}{\\mb_N}{\\Sb_N}\n",
    "\\\\\n",
    "\\text{where }\n",
    "\\\\\n",
    "\\mb_N &= \\Sb_N \\left( \\Sb_0^{-1} \\mb_0 + \\beta \\Phib^T \\Phib \\right)\n",
    "\\\\\n",
    "\\Sb_{N}^{-1} &= \\Sb_{0}^{-1} + \\beta \\Phib^T \\Phib\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using <a href='../Gaussian Stuff.ipynb#BayesTheoremForGaussianVariables'>Bayes' Theorem for Gaussian Variables</a>, we have the following\n",
    "\n",
    "Given a marginal Gaussian for **x** and a conditional gaussian for **y** given **x** of the form\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\xb) \n",
    "&=\n",
    "\\Nl{\\xb}{\\mub}{\\li}\n",
    "\\\\\n",
    "p(\\yb \\mid \\xb)\n",
    "&=\n",
    "\\Nl{\\yb}{\\Ab\\xb + \\bb}{\\Lbi}\n",
    "\\\\\n",
    "\\color{green}{\\text{Marginal }}\n",
    "p(\\yb)\n",
    "&=\n",
    "\\Nl{\\yb}{\\Ab\\mub+\\bb}{\\Lbi + \\Ab\\Lambda^{-1}\\Abt}\n",
    "\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "Here\n",
    "$$\n",
    "\\arrthree{\n",
    "\\xb &\\equiv \\wb\n",
    "&\n",
    "\\mub &\\equiv \\mb_N\n",
    "&\n",
    "\\li &\\equiv \\Sb_N\n",
    "\\\\\n",
    "\\yb &\\equiv t\n",
    "&\n",
    "\\Abt,\\bb &\\equiv \\Phib,\\mathbf{0}\n",
    "&\n",
    "\\Lbi &\\equiv \\beta^{-1}\n",
    "}\n",
    "$$\n",
    "\n",
    "Hence\n",
    "$$\n",
    "\\arrthree{\n",
    "\\E{\\yb}\n",
    "&=\n",
    "\\Ab \\mub + \\bb = \n",
    "\\mb_N^T \\Phib\n",
    "\\\\\n",
    "\\sigma(\\yb)\n",
    "&=\n",
    "\\beta^{-1} + \\Phib^T \\Sb_N \\Phib\n",
    "}\n",
    "$$\n",
    "\n",
    "The second term goes to zero as N increases ([Qazaz][qazaz1997]).\n",
    "\n",
    "[qazaz1997]: http://dl.acm.org/citation.cfm?id=268081 \"Cambridge University Press. Qazaz, C. S., C. K. I. Williams, and C. M. Bishop (1997). An upper bound on the Bayesian error bars for generalized linear regression. In S. W. Ellacott, J. C. Mason, and I. J. Anderson (Eds.), Mathematics of Neural Networks: Models, Algorithms and Applications, pp. 295–299. Kluwer.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat both $\\wb, \\beta$ as unknown, the predictive distribution becomes a Student't t-distribution (<a href='/notebooks/void-main/Gaussian%20Stuff.ipynb#Unknown-mean,-unknown-variance'>Unknown Mean,Variance</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nosNfczH9pgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda4ee72da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample theta from [0,2*pi]\n",
    "in_samples, in_pts_pred=10,10**2\n",
    "thetas_sample = np.random.rand(in_samples)*2*math.pi\n",
    "thetas = np.linspace(0,2*math.pi,in_pts_pred)\n",
    "plt.plot(thetas, [math.sin(theta) for theta in thetas],label='sine')\n",
    "\n",
    "# adding noise\n",
    "beta_sd = 0.2\n",
    "beta = 1./(beta_sd**2)\n",
    "xn_noise = np.random.normal(0,beta_sd,in_samples)\n",
    "tn = [[math.sin(theta) for theta in thetas_sample]+xn_noise]\n",
    "tn = np.matrix(tn).reshape(in_samples,1)\n",
    "plt.plot(thetas_sample, tn, 'or',label='tn')\n",
    "\n",
    "alpha=2.0\n",
    "phi_posterior = np.matrix(\n",
    "    np.vstack(\n",
    "        [np.ones_like(thetas_sample),\n",
    "         thetas_sample\n",
    "        ]\n",
    "    )\n",
    ").transpose()\n",
    "# Sn = beta*phi^T*phi + alpha*I\n",
    "sn_inv = beta * phi_posterior.transpose() * phi_posterior + alpha*np.eye(2)\n",
    "sn = np.linalg.inv(sn_inv)\n",
    "# m_n = beta*Sn*Phi^T*Tn\n",
    "mean_n = beta * sn * phi_posterior.transpose() * tn\n",
    "\n",
    "phi_pred = np.matrix(\n",
    "    np.vstack(\n",
    "        [np.ones_like(thetas), thetas]))\n",
    "mean_pred = mean_n.transpose() * phi_pred\n",
    "plt.plot(np.matrix(thetas).transpose(), mean_pred.transpose(),label='predicted mean')\n",
    "\n",
    "covar_pred = 1./beta + phi_pred.transpose()*sn*phi_pred\n",
    "print(covar_pred.shape)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent Kernel\n",
    "-----------------\n",
    "\n",
    "* The posterior mean (<a href='#BayesianLinearRegressionPosteriorParameters'>Posterior Mean</a>) is given by $\\mb_N = \\beta \\Sb_N \\Phib^T \\tb$.\n",
    "* Sub this into the regression function, we have\n",
    "$$\n",
    "\\arrthree{\n",
    "y(\\xb, \\mb_N)\n",
    "&=\n",
    "\\mb_N^T \\phi(x)\n",
    "\\\\\n",
    "&=\n",
    "\\phi(x) \\mb_N^T\n",
    "\\\\\n",
    "&=\n",
    "\\beta \\phi(\\xb)^T \\Sb_N \\Phib^T \\tb\n",
    "\\\\\n",
    "&=\n",
    "\\beta \\phi(\\xb)^T \\Sb_N\n",
    "\\mat{\n",
    "\\vdots      & \\cdots & \\vdots \\\\\n",
    "\\phi(\\xb_1) & \\cdots & \\phi(\\xb_N) \\\\\n",
    "\\vdots      & \\cdots & \\vdots \\\\\n",
    "}\n",
    "\\tb\n",
    "\\\\\n",
    "&=\n",
    "\\sumnN \\beta \\phi(\\xb)^T \\Sb_N \\phi(\\xb_n) t_n\n",
    "\\\\\n",
    "&=\n",
    "\\sumnN k(\\xb,\\xb_n) t_n\n",
    "\\\\\n",
    "\\text{where }\\\\\n",
    "k(\\xb,\\xb^{\\prime})\n",
    "&=\n",
    "\\beta \\phi(\\xb)^T \\Sb_N \\phi(\\xb_n)\n",
    "}\n",
    "$$\n",
    "\n",
    "* *k* is called *smoother matrix* or the *equivalent kernel*\n",
    "* *Linear Smoothers*: Makes predictions by taking linear combination of the training set target values\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\sigma[y(\\xb),y(\\xb^{\\prime})]\n",
    "&=\n",
    "\\sigma[\\phi(\\xb)^T\\wb, \\wb^T \\phi(\\xb^{\\prime})]\n",
    "\\\\\n",
    "&=\n",
    "\\phi(\\xb)^T \\Sb_N \\phi(\\xb^{\\prime})\n",
    "\\\\\n",
    "&=\n",
    "\\beta^{-1} k(\\xb, \\xb^{\\prime})\n",
    "}\n",
    "$$\n",
    "[? How the fuck did you get to that second step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence Approximation\n",
    "=======================\n",
    "\n",
    "* There are two hyperparameters, $\\alpha$ and $\\beta$\n",
    " * $\\alpha$: for the prior of $\\wb$\n",
    " * $\\beta$ for the noise of y\n",
    " \n",
    "* Can integrate analytically over **w** or hyperparameters\n",
    "  * but complete marginalization is analytically intractable\n",
    "  \n",
    "* Here, we use an approximation to find the hyperparameters by maximizing marginal likelihood function obtained by first integrating over **w**\n",
    "* This framework is called\n",
    "  * empirical Bayes\n",
    "  * type 2 maximum likelihood\n",
    "  * generalized maximum likelihood\n",
    "  * evidence approximation\n",
    "  \n",
    "Steps involved\n",
    "1. use the prior and likelihood forms\n",
    "1. complete the square to separate out **w**\n",
    "1. use normalization coefficient of a Gaussian\n",
    "1. compute the log evidence\n",
    "1. find $\\alpha$ and $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive distribution:\n",
    "$$\n",
    "p(t \\mid \\tb) = \n",
    "\\iiint\n",
    "~p(t \\mid \\wb, \\beta)\n",
    "~p(\\wb \\mid \\tb, \\alpha, \\beta)\n",
    "~p(\\alpha, \\beta \\mid \\tb)\n",
    "~d\\wb ~d\\alpha ~d\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal likelihood:\n",
    "$$\n",
    "p(\\tb \\mid \\alpha, \\beta) = \n",
    "\\int\n",
    "~p(\\tb \\mid \\wb, \\alpha, \\beta)\n",
    "~p(\\wb \\mid \\alpha)\n",
    "~d\\wb\n",
    "$$\n",
    "\n",
    "* this can be evaluated using the [conditional distribution](/notebooks/void-main/Gaussian%20Stuff.ipynb#Conditional-Gaussian-Distributions) of a gaussian model\n",
    "* we shall evaluate by completing the integral and making use of the std. form of the normalization coefficient of the gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional likelihood:\n",
    "$$\n",
    "p(\\tb \\mid \\wb, \\beta) = \n",
    "\\mathcal{N}(t_n \\mid \\wbt \\phi(\\xb_n), \\beta^{-1})\n",
    "=\n",
    "\\frac{N}{2} \\ln \\beta\n",
    "-\\frac{N}{2} \\ln(2\\pi)\n",
    "-\\beta E_D(\\wb)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "E_D(\\wb) = \n",
    "\\frac12 \\sumnN\n",
    "\\left\\{\n",
    "  t_n - \\wbt \\phi(\\xb_n)\n",
    "\\right\\}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior:\n",
    "$$\n",
    "p(\\wb \\mid \\alpha) = \n",
    "\\mathcal{N}(\\wb \\mid \\mathbf{0}, \\alpha^{-1} \\mathbf{I})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence,\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\tb \\mid \\alpha, \\beta) &=\n",
    "\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\n",
    "\\left(\\frac{\\alpha}{2\\pi}\\right)^{M/2}\n",
    "\\int \\exp\\{-E(\\wb)\\} ~d\\wb\n",
    "\\\\\n",
    "\\text{where} \\\\\n",
    "E(\\wb)\n",
    "&=\n",
    "\\beta E_D(\\wb) + \\alpha E_W(\\wb)\n",
    "\\\\ &=\n",
    "\\beta \\normsqr{\\tb - \\Phi \\wb} +\n",
    "\\frac{\\alpha}{2} \\wbt \\wb\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "E(\\wb)\n",
    "&=\n",
    "\\beta \\normsqr{\\tb - \\Phi \\wb} + \\frac{\\alpha}{2} \\wbt \\wb\n",
    "\\\\ &=\n",
    "\\beta (\\tb - \\Phi \\wb)^T (\\tb - \\Phi \\wb) + \\frac{\\alpha}{2} \\wbt \\wb\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\beta\n",
    "\\left(\n",
    "  \\tb^T \\tb\n",
    "- \\tb^T \\Phi \\wb\n",
    "- \\wbt \\Phi^T \\tb\n",
    "+ \\wbt \\Phi^T \\Phi \\wb\n",
    "\\right)\n",
    "+ \\frac{\\alpha}{2} \\wbt \\wb\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a lotta shit,\n",
    "$$\n",
    "\\arrthree{\n",
    "E(\\wb) &=\n",
    "E(\\mb_N) + \\frac12 (\\wb-\\mb_N)^T \\Ab (\\wb -\\mb_N)\n",
    "\\\\\n",
    "\\text{where}\n",
    "\\\\\n",
    "\\Ab &= \\alpha \\Ib + \\beta \\Phi^T \\Phi\n",
    "\\\\\n",
    "\\mb_N &= \\beta \\Ab^{-1} |Phi^T \\tb\n",
    "\\\\\n",
    "E(\\mb_N) &=\n",
    "\\frac{\\beta}{2} \\normsqr{\\tb - \\Phi \\mb_N} \n",
    "+\\frac{\\alpha}{2} \\mb_{N}^{T} \\mb_N\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$$\n",
    "\\arrthree{\n",
    "\\int &\\exp \\left\\{ -E(\\wb) \\right\\} ~d\\wb\n",
    "\\\\ &=\n",
    "\\exp\\left\\{ -E(\\mb_N)\\right\\}\n",
    "\\int \\exp\n",
    "\\left\\{\n",
    "  -\\frac12 (\\wb - \\mb_N)^T \\Ab (\\wb - \\mb_N)\n",
    "\\right\\}\n",
    "~d\\wb\n",
    "\\\\ \n",
    "&=\n",
    "\\exp\\left\\{ -E(\\mb_N)\\right\\}\n",
    "(2\\pi)^{M/2}\n",
    "\\left| \\Ab \\right|^{-1/2}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$$\n",
    "\\arrthree{\n",
    "\\ln p(\\tb \\mid \\alpha, \\beta)\n",
    "&=\n",
    "\\frachalf{M} \\ln \\alpha\n",
    "+ \\frachalf{N} \\ln \\beta\n",
    "- E(\\mb_N)\n",
    "- \\frac12 \\ln \\left|\\Ab\\right|\n",
    "- \\frac12 \\ln(2\\pi)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding alpha\n",
    "-------------------------\n",
    "\n",
    "Consider the eigenvector equation\n",
    "$$\n",
    "\\left(\n",
    "  \\beta \\Phi^T \\Phi\n",
    "\\right)\n",
    "\\ub_i\n",
    "=\n",
    "\\lambda_i \\ub_i\n",
    "$$\n",
    "Hence the eigenvalues of **A** is $\\alpha + \\lambda_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\frac{d}{d\\alpha} \\ln \\left| \\Ab \\right|\n",
    "&=\n",
    "\\frac{d}{d\\alpha} \\ln \\prod_i \\left( \\alpha + \\lambda_i \\right)\n",
    "\\\\ &=\n",
    "\\frac{d}{d\\alpha} \\sum_i \\ln \\left( \\alpha + \\lambda_i \\right)\n",
    "\\\\ &=\n",
    "\\sum_i \\frac{1}{\\alpha + \\lambda_i}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, diff ln p(**t** | $\\alpha,\\beta$) wrt $\\alpha$\n",
    "$$\n",
    "\\arrthree{\n",
    "0 &=\n",
    "\\frac{M}{2\\alpha}\n",
    "- \\frac12 \\mb_N^T \\mb_N\n",
    "- \\frac12 \\sum_i \\frac{1}{\\alpha + \\lambda_i}\n",
    "\\\\\n",
    "\\alpha \\mb_N^T \\mb_N\n",
    "&=\n",
    "M - \\alpha \\sum_i \\frac{1}{\\alpha + \\lambda_i}\n",
    "\\\\\n",
    "&=\n",
    "\\sum_i 1\n",
    "- \\sum_i \\frac{\\alpha}{\\alpha + \\lambda_i}\n",
    "\\\\ &=\n",
    "\\sum_i 1 - \\frac{\\alpha}{\\alpha + \\lambda_i}\n",
    "\\\\ &=\n",
    "\\sum_i \\frac{\\lambda_i}{\\alpha + \\lambda_i}\n",
    "\\\\ &=\n",
    "\\gamma\n",
    "\\\\ \\text{Thus,} \\\\\n",
    "\\alpha &= \\frac{\\gamma}{\\mb_N^T \\mb_N}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is an implicit solution for $\\alpha$\n",
    "* have to resort to an iterative procedure to find $\\alpha$ and $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding beta\n",
    "----------------\n",
    "\n",
    "* Eigenvalues $\\lambda_i$ are proportional to $\\beta$\n",
    "* Hence $$d\\lambda_i/d\\beta = \\lambda_i / \\beta$$\n",
    "Hence,\n",
    "$$\n",
    "\\arrthree{\n",
    "\\frac{d}{d\\beta}\n",
    "&= \n",
    "\\ln \\left| \\Ab \\right|\n",
    "\\\\\n",
    "&=\n",
    "\\frac{d}{d\\beta}\n",
    "\\ln \\prod_i (\\alpha + \\lambda_i)\n",
    "\\\\ &=\n",
    "\\frac{d}{d\\beta}\n",
    "\\sum_i \\ln (\\alpha + \\lambda_i)\n",
    "\\\\ &=\n",
    "\\frac{1}{\\beta} \\sum_i \\frac{\\lambda_i}{\\lambda_i + \\alpha}\n",
    "\\\\ &=\n",
    "\\frac{\\gamma}{\\beta}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, diff ln p(**t** | $\\alpha,\\beta$) wrt $\\beta$\n",
    "$$\n",
    "\\arrthree{\n",
    "0 &=\n",
    "\\frac{N}{2\\beta}\n",
    "-\\frac12 \\sumnN\n",
    "\\left\\{\n",
    "  t_n - \\mb_N^T \\phi(x_n)\n",
    "\\right\\}^2\n",
    "- \n",
    "\\frac{\\gamma}{2\\beta}\n",
    "\\\\\n",
    "\\frac{1}{\\beta}\n",
    "&=\n",
    "\\frac{1}{N-\\gamma}\n",
    "-\\frac12 \\sumnN\n",
    "\\left\\{\n",
    "  t_n - \\mb_N^T \\phi(x_n)\n",
    "\\right\\}^2\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is an implicit solution for $\\beta$ and we have to resort to an iterative procedure to find its value.\n",
    "\n",
    "Thanks and Regards,  \n",
    "Your soul reaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
