{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "$ \\newcommand{\\E}[1]{\\mathbb{E}\\left[#1\\right]}$  \n",
    "$ \\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}$\n",
    "$ \\newcommand{\\EXP}[1]{\\exp\\left(#1\\right)}$  \n",
    "$ \\newcommand{\\P}{\\mathbb{P}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**  \n",
    "1. Use different basis models for the same problem\n",
    "1. Compare w/ and w/o regularization\n",
    "1. Use different regularizers for the same problem\n",
    "1. Bias Variance Decomposition + Experiments\n",
    "\n",
    "**Questions**\n",
    "1. Equation \\ref{eq:ptw} under ML and Least Squares\n",
    "1. Why does Lasso act as a feature selector?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "=======\n",
    "\n",
    "Simple Linear Model\n",
    "------------------\n",
    "\n",
    "y(\\textbf{x},**w**) $ = w_0 + \\sum_{i=1}^{D} w_i x_i$ \n",
    "\n",
    "where  \n",
    "**x** $ = (x_1, \\cdots, x_D)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Functions\n",
    "---------------\n",
    "\n",
    "$\n",
    "y(\\mathbf{x},\\mathbf{w}) = w_0 + \\sum_{j=1}^{M-1} w_i ~ \\phi_j(\\mathbf{x})\n",
    "$ \n",
    "\n",
    "* where $\\phi_j(\\mathbf{x})$ are called basis functions. \n",
    "* Total #parameters = M\n",
    "\n",
    "If $\\phi_0(\\mathbf{x}) = 1$, then  \n",
    "$y(\\mathbf{x},\\mathbf{w})  = \\sum_{j=0}^{M-1} w_i ~ \\phi_j(\\text{x}) = \\mathbf{w}^T \\phi(\\mathbf{x})$ \n",
    "\n",
    "* Linear in **w**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choices of Basis Functions\n",
    "-------------------------\n",
    "\n",
    "1. Polynomial Basis\n",
    "  * $\\phi_j(\\mathbf{x}) = x^j$\n",
    "  * Limitation: Global models\n",
    "  * Cure: Spline Functions: fit different polynomials based on region [EOSL Hastie]\n",
    "1. Gaussian Basis FUnction:\n",
    "  * $\\phi_j(\\mathbf{x}) = \\EXP{-\\frac{(x-\\mu_j)^2}{2s^2}}$\n",
    "  * Need not be a pdf\n",
    "1. Sigmoidal\n",
    "  * $\\phi_j(\\mathbf{x}) = \\sigma \\left( \\frac{x-\\mu_j}{s} \\right)$\n",
    "  * where $\\sigma(a) = \\frac{1}{1+\\EXP{-a}}$ is the logistic sigmoid function\n",
    "  * Can use $\\tanh$, since $\\tanh(a) = 2\\sigma(2a) - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions for Regression\n",
    "=================\n",
    "\n",
    "[PRML]1.5.5\n",
    "\n",
    "Say we fit a function y(x) to give the target variable t. \n",
    "Let $\\mathcal{L}(t, y(x))$ denote the loss function.\n",
    "\n",
    "Then, Average or Expected Loss is given by,\n",
    "$\\E{\\mathcal{L}} = \\iint \\mathcal{L}(t, y(x)) ~p(x,t) ~dx ~dt$\n",
    "\n",
    "If we consider the squared loss function $\\mathcal{L}(t, y(\\mathbf{x})) = \\left( t - y(\\mathbf{x})\\right)^2$, then,\n",
    "\n",
    "\\begin{array}{llr}\n",
    "\\E{\\mathcal{L}} \n",
    "&= \\iint \\left( t - y(\\mathbf{x})\\right)^2 ~p(\\mathbf{x},t) ~d\\mathbf{x} ~dt\n",
    "\\\\\n",
    "\\frac{\\partial \\E{\\mathcal{L}}}{\\partial y(\\mathbf{x})}\n",
    "&=\n",
    "2 \\int \\left( t - y(\\mathbf{x})\\right) ~p(\\mathbf{x},t) ~dt = 0\n",
    "\\\\\n",
    "\\int t ~p(\\mathbf{x},t) ~dt  \n",
    "&=\n",
    "\\int y(\\mathbf{x}) ~p(\\mathbf{x},t) ~dt = y(\\mathbf{x}) \\int  ~p(\\mathbf{x},t) ~dt\n",
    "\\\\\n",
    "\\int t ~p(\\mathbf{x},t) ~dt  \n",
    "&=\n",
    "y(\\mathbf{x}) p(\\mathbf{x})\n",
    "\\\\\n",
    "y(\\mathbf{x})\n",
    "&= \n",
    "\\frac{\\int t ~p(\\mathbf{x},t) ~dt  }{p(\\mathbf{x})}\n",
    "= \\int t ~p(t \\mid \\mathbf{x}) ~dt\n",
    "\\\\\n",
    "y(\\mathbf{x}) \n",
    "&= \\mathbb{E}_{t} \\left[ t \\mid \\mathbf{x} \\right]\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt. Derivation   \n",
    "\n",
    "$t = \\E{\\E{t \\mid \\mathbf{x}}}$\n",
    "hence, $\\E{\\E{t \\mid \\mathbf{x}} - t}^2 = \\V{t \\mid \\mathbf{x}}$ \n",
    "\n",
    "\\begin{array}{llr}\n",
    "\\left( y(\\mathbf{x}) - t \\right)^2\n",
    "&=\n",
    "\\left( y(\\mathbf{x}) - \\E{t \\mid X} + \\E{t \\mid \\mathbf{x}} - t \\right)^2\n",
    "\\\\\n",
    "&=\n",
    "\\left(\n",
    "    y(\\mathbf{x}) - \\E{t \\mid \\mathbf{x}} \n",
    "\\right)^2 \n",
    "+\n",
    "2\n",
    "\\left(\n",
    "    y(\\mathbf{x}) - \\E{t \\mid \\mathbf{x}}\n",
    "\\right)\n",
    "\\left(\n",
    "    \\E{t \\mid \\mathbf{x}} - t\n",
    "\\right)\n",
    "+\n",
    "\\left(\n",
    "    \\E{t \\mid \\mathbf{x}} - t\n",
    "\\right)^2\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting into $\\E{\\mathcal{L}}$, the cross term vanishes (the first term of the cross term). Hence  \n",
    "$$\n",
    "\\E{\\mathcal{L}} =\n",
    "\\underbrace{\\int\n",
    "\\left(\n",
    "    y(\\mathbf{x}) - \\E{t \\mid \\mathbf{x}} \n",
    "\\right)^2 ~p(\\mathbf{x}) ~d\\mathbf{x}\n",
    "}_{\\text{Bias Term}}\n",
    "-\n",
    "\\underbrace{\n",
    "\\int\n",
    "\\V{t \\mid \\mathbf{x}} ~p(\\mathbf{x}) ~d\\mathbf{x}\n",
    "}_{\\text{Variance term}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML and Least Squares\n",
    "===========\n",
    "\n",
    "Let *t* be given by a deterministic *y* and additive Gaussian noise as,\n",
    "$t = y(\\mathbf{x},\\mathbf{w}) + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$  \n",
    "Then,  \n",
    "$$\n",
    "p(t \\mid \\mathbf{x}, \\mathbf{w}, \\beta) = \\mathcal{N}(t \\mid y(\\mathbf{x},\\mathbf{w}), \\beta^{-1})\n",
    "\\label{eq:ptw}\n",
    "$$\n",
    "[?]  \n",
    "If we assume a squared loss function, optimal t is given by conditional mean of t. In case of a Gaussian, it becomes\n",
    "$\\E{t \\mid \\mathbf{x}} = \\int t ~p(t \\mid \\mathbf{x}) ~dt = y(\\mathbf{x},\\mathbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $\\mathbf{x}$ is IID Normal given by $\\ref{eq:ptw}$, then  \n",
    "$$\n",
    "p(\\mathbf{t} \\mid \\mathbf{X}, \\mathbf{w}, \\beta)\n",
    "=\n",
    "\\prod_{n=1}^{N}\n",
    "\\mathcal{N}\n",
    "    \\left(\n",
    "        t_n \\mid \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x_n}), \\beta^{-1}\n",
    "    \\right)\n",
    "\\label{eq:ptiid}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the log likehood, we get  \n",
    "\\begin{array}{ll}\n",
    "\\ln p(\\mathbf{t} \\mid \\mathbf{w}, \\beta)\n",
    "&=\n",
    "\\frac{N}{2} \\ln \\beta\n",
    "-\n",
    "\\frac{N}{2} \\ln(2\\pi)\n",
    "-\n",
    "\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Estimation\n",
    "-------------------\n",
    "Grad of the log likehood (wrt $\\mathbf{w}$) gives,  \n",
    "\\begin{array}{ll}\n",
    "\\nabla \\ln p(\\mathbf{t} | \\mathbf{w}, \\beta)\n",
    "&=\n",
    "\\beta\n",
    "\\sum_{n=1}^{N}\n",
    "(t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "= 0\n",
    "\\\\\n",
    "\\sum_{n=1}^{N}\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "-\n",
    "\\mathbf{w}^T \n",
    "\\sum_{n=1}^{N}\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "&= 0\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\phi}\n",
    "=\n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "        \\phi_1\\\\\n",
    "        \\phi_2\\\\\n",
    "        \\vdots\\\\\n",
    "        \\phi_{M-1}\n",
    "    \\end{matrix}\n",
    "\\right]\n",
    "\\hspace{5pt}\n",
    "\\mathbf{t}\n",
    "=\n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "        t_1\\\\\n",
    "        t_2\\\\\n",
    "        \\vdots\\\\\n",
    "        t_{N}\n",
    "    \\end{matrix}\n",
    "\\right]\n",
    "\\hspace{20pt}\n",
    "\\mathbf{\\phi} \\mathbf{\\phi}^T\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\phi_0 \\phi_0 & \\phi_0 \\phi_1 & \\cdots & \\phi_0 \\phi_{M-1}\\\\\n",
    "\\phi_1 \\phi_0 & \\phi_1 \\phi_1 & \\cdots & \\phi_1 \\phi_{M-1}\\\\\n",
    "\\vdots        & \\vdots        & \\ddots       & \\vdots\\\\\n",
    "\\phi_{M-1} \\phi_0 & \\phi_{M-1} \\phi_1 & \\cdots & \\phi_{M-1} \\phi_{M-1}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we let\n",
    "$$\n",
    "\\Phi(\\mathbf{X})\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\phi_0(x_1) & \\phi_1(x_1) & \\cdots & \\phi_{M-1}(x_1)\\\\\n",
    "\\phi_0(x_2) & \\phi_1(x_2) & \\cdots & \\phi_{M-1}(x_2)\\\\\n",
    "\\vdots      & \\vdots      & \\ddots & \\vdots\\\\\n",
    "\\phi_0(x_N) & \\phi_1(x_N) & \\cdots & \\phi_{M-1}(x_N)\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N}\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) ~ \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "=\n",
    "\\Phi(\\mathbf{X})^T \\Phi(\\mathbf{X})\n",
    "\\\\\n",
    "\\sum_{n=1}^{N}\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "=\n",
    "\\Phi(\\mathbf{X})^T \\mathbf{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$\n",
    "\\mathbf{w}_{ML} = \\left(\\Phi^T \\Phi \\right)^{-1} \\Phi^T \\mathbf{t}\n",
    "$\n",
    "which is called *normal equations* for least squares.  \n",
    "$\\Phi$ is called the *design matrix* which is $N \\times M$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Parameter\n",
    "---------------\n",
    "\n",
    "Now, consider the last term and seperate out the bias parameter\n",
    "$\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - w_0 - \\sum_{m=1}^{M-1} w_m \\phi_m(\\mathbf{x}_n)\n",
    "\\right)^2$  \n",
    "Diff wrt $w_0$ and equating it to zero, we get  \n",
    "\n",
    "\\begin{array}{ll}\n",
    "0\n",
    "&=\n",
    "\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "2(t_n - w_0 - \\sum_{m=1}^{M-1} w_m \\phi_m(\\mathbf{x}_n))\n",
    "\\\\\n",
    "N w_0\n",
    "&=\n",
    "\\sum_{n=1}^{N}\n",
    "t_n\n",
    "-\n",
    "\\sum_{n=1}^{N}\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m \\phi_m(\\mathbf{x}_n)\n",
    "\\\\\n",
    "N w_0\n",
    "&=\n",
    "\\sum_{n=1}^{N}\n",
    "t_n\n",
    "-\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m \n",
    "\\sum_{n=1}^{N}\n",
    "\\phi_m(\\mathbf{x}_n)\n",
    "\\\\\n",
    "w_0\n",
    "&=\n",
    "\\overline{t}_n\n",
    "-\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m\n",
    "\\overline{\\phi}_m\n",
    "\\end{array}\n",
    "\n",
    "where  \n",
    "$\n",
    "\\overline{t}_n = \\frac{1}{N} \\sum_{n=1}^{N} t_n \\\\\n",
    "\\overline{\\phi}_m = \\frac{1}{N} \\sum_{n=1}^{N} \\phi_m(\\mathbf{x}_n) \\\\\n",
    "$\n",
    "\n",
    "Thus the basis $w_0$ compensates for the difference between the average of target values\n",
    "and\n",
    "the weighted sum of the average of the basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameter\n",
    "--------------\n",
    "\n",
    "Diff log likelihood wrt $\\beta$ and equating it to zero, we get  \n",
    "\\begin{array}{ll}\n",
    "0\n",
    "&=\n",
    "\\frac{N}{2}\n",
    "\\frac{1}{\\beta}\n",
    "-\n",
    "\\frac{1}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\\\\\n",
    "\\frac{1}{\\beta_{ML}}\n",
    "&=\n",
    "\\frac{1}{N}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Learning\n",
    "===========\n",
    "\n",
    "* Stochastic Gradient can be used to find the parameters sequentially.\n",
    "* Update rule: $\\mathbf{w}_{\\tau+1} = \\mathbf{w}_{\\tau} + \\eta \\nabla E(\\mathbf{w})$\n",
    "* If squared loss function is assumed,  \n",
    "  $\\nabla E_D(\\mathbf{w}) = \n",
    "  \\left( \n",
    "      t_n - \n",
    "      \\mathbf{w}_{\\tau}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "  \\right)\n",
    "  \\mathbf{\\phi}(\\mathbf{x}_n)$\n",
    "* Hence, the update rule becomes,  \n",
    "  $\\mathbf{w}_{\\tau+1}\n",
    "  = \\mathbf{w}_{\\tau}\n",
    "    + \\eta\n",
    "      \\left( \n",
    "          t_n - \n",
    "          \\mathbf{w}_{\\tau}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "      \\right)\n",
    "      \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "========\n",
    "\n",
    "* Form: $E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Regularization\n",
    "-----------------------\n",
    "\n",
    "* $E_W(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^T \\mathbf{w}$\n",
    "* Called as *weight decay* since $\\mathbf{w}$ decays to zero when $\\lambda$ is high\n",
    "* Called as *Parameter Shrinkage* as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total Error Function:  \n",
    "\\begin{array}{rlr}\n",
    "E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w})\n",
    "&=\n",
    "\\frac{1}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "(t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "+\n",
    "\\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}\n",
    "\\\\\n",
    "0\n",
    "&=\n",
    "\\sum_{n=1}^{N}\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "-\n",
    "\\mathbf{w}^T \n",
    "\\sum_{n=1}^{N}\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "+ \\lambda \\mathbf{w}^T\n",
    "& \\color{gray}{\\text{Diff wrt w}}\n",
    "\\\\\n",
    "0\n",
    "&=\n",
    "\\Phi^T \\mathbf{t} - \\mathbf{w}^T \\Phi^T \\Phi + \\lambda \\mathbf{w}^T\n",
    "\\\\\n",
    "\\left( \\Phi^T \\Phi - \\lambda \\mathcal{I} \\right) \\mathbf{w} \n",
    "&=\n",
    "\\Phi^T \\mathbf{t}\n",
    "\\\\\n",
    "\\mathbf{w}\n",
    "&= \n",
    "\\left( \n",
    "    \\Phi^T \\Phi - \\lambda \\mathcal{I}\n",
    "\\right)^{-1} \n",
    "\\Phi^T \\mathbf{t}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalized Regularizer\n",
    "---------------------\n",
    "* $\\frac{\\lambda}{2} \\sum_{m=0}^{M-1} \\left| \\mathbf{w_j} \\right|^q$\n",
    "* Allows complex models to be fit on smaller data sets\n",
    "* The problem of finding the right model complexity transforms to that of finding the right value for $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso\n",
    "-----\n",
    "\n",
    "* By setting $q=1$ in the generalized regularizer\n",
    "* Has a tendency to set some weights to zero, thereby making it a \"feature selector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Outputs\n",
    "==========\n",
    "\n",
    "* Let\n",
    "  * $\\mathbf{y}(\\mathbf{x},\\mathbf{w})$ be K dimensional\n",
    "  * W be $M \\times K$ matrix of parameters\n",
    "  * $\\mathbf{\\phi}(\\mathbf{x})$ is M dimensional\n",
    "  * $\\mathbf{t}$ is K dimensional\n",
    "\n",
    "\\begin{array}{rlr}\n",
    "p(\\mathbf{t} \\mid \\mathbf{x}, \\mathbf{W}, \\beta)\n",
    "&=\n",
    "\\mathcal{N}\n",
    "\\left(\n",
    "\\mathbf{t} \\mid \\mathbf{W}^T \\mathbf{\\phi}(\\mathbf{x}),\n",
    "\\beta^{-1} \\mathcal{I} \n",
    "\\right)\n",
    "\\\\\n",
    "\\ln p = 0\n",
    "&=\n",
    "\\frac{NK}{2} \\ln\\left(\\frac{\\beta}{2\\pi}\\right)\n",
    "-\n",
    "\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left\\|\n",
    "\\mathbf{t}_n\n",
    "-\n",
    "\\mathbf{W}^T \\mathbf{\\phi}(\\mathbf{x})\n",
    "\\right\\|^2\n",
    "&\n",
    "\\color{gray}{\\text{See: Multivariate Gaussian}}\n",
    "\\\\\n",
    "\\mathbf{W}_{ML}\n",
    "&=\n",
    "\\left(\n",
    "\\Phi^T \\Phi\n",
    "\\right)^{-1}\n",
    "\\Phi^T \\mathbf{T}\n",
    "= \n",
    "\\Phi^{\\dagger} \\mathbf{T}\n",
    "\\end{array}\n",
    "* That is, the same $\\Phi^{\\dagger}$ can be used for all K output target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Variance Decomposition\n",
    "===============\n",
    "\n",
    "* Frequentist viewpoint of the model complexity issue\n",
    "* For a squared loss function, the optimal prediction is given by the conditional expectation h(x),  \n",
    "  $h(\\mathbf{x}) = \\E{t \\mid \\mathbf{x}} = \\int t ~p (t \\mid \\mathbf{x}) ~dt$\n",
    "* The expected square loss can be written as,\n",
    "$$\n",
    "\\E{\\mathcal{L}}\n",
    "=\n",
    "\\int \\left(\n",
    "    y(\\mathbf{x}) - h(\\mathbf{x})\n",
    "\\right)^2\n",
    "~p(\\mathbf{x}) ~d\\mathbf{x}\n",
    "+\n",
    "\\iint \\left(\n",
    "    h(\\mathbf{x}) - t\n",
    "\\right)^2\n",
    "~p(\\mathbf{x}, t) ~dx dt\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init\n",
    "$\\newcommand{\\xb}{\\mathbf{x}}$\n",
    "$\\newcommand{\\yx}{y(\\xb; \\mathcal{D})}$\n",
    "$\\newcommand{\\hx}{h(\\xb)}$\n",
    "$\\newcommand{\\ed}[1]{\\mathbb{E}_D\\left[ #1 \\right]}$\n",
    "$\\newcommand{\\edyx}{\\ed{\\yx}}$\n",
    "$\\newcommand{\\px}{~p(\\xb)}$\n",
    "$\\newcommand{\\dx}{~d\\xb}$\n",
    "$\\newcommand{\\pxdx}{\\px \\dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the First term $\\left(y(\\mathbf{x}) - h(\\mathbf{x})\\right)^2$.  \n",
    "$\\pm \\edyx$, we get\n",
    "$$\n",
    "\\left\\{\\yx - \\edyx + \\edyx - \\hx\\right\\}^2\n",
    "\\\\\n",
    "\\hspace{10pt}=\n",
    "\\left(\n",
    "    \\yx - \\edyx\n",
    "\\right)^2\n",
    "+\n",
    "\\left(\n",
    "    \\edyx - \\hx\n",
    "\\right)^2\n",
    "+\n",
    "2\n",
    "\\left(\\yx - \\edyx\\right)\n",
    "\\left(\\edyx - \\hx\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the expectation wrt $\\mathcal{D}$, the last term vanishes.  \n",
    "$$\n",
    "\\ed{\\left(\\yx - \\hx\\right)^2}\n",
    "\\\\\n",
    "=\n",
    "\\underbrace{\\left(\n",
    "    \\ed{\\yx - \\hx}\n",
    "\\right)^2}_{(\\text{bias})^2}\n",
    "+\n",
    "\\underbrace{\n",
    "\\ed{\n",
    " \\left(\n",
    "     \\yx - \\edyx\n",
    " \\right)\n",
    "}}_{\\text{variance}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Squared-bias: Deviation of the average prediction from the desired\n",
    "1. Variation of the individual predictions about the average prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,  \n",
    "\\begin{array}{ll}\n",
    "\\text{expected loss} &= (\\text{bias})^2 + \\text{variance} + \\text{noise}\\\\\n",
    "&\\color{green}{\\text{where}}&\\\\\n",
    "(\\text{bias})^2\n",
    "&=\n",
    "\\int \\left(\\edyx - \\hx\\right)^2 \\pxdx\n",
    "\\\\\n",
    "\\text{variance}\n",
    "&=\n",
    "\\int \\ed{\\left( \\yx - \\edyx \\right)^2} \\pxdx\n",
    "\\\\\n",
    "\\text{noise}\n",
    "&=\n",
    "\\iint \\left(\\hx - t\\right)^2 ~p(\\xb,t) \\dx ~dt\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Value**  \n",
    "* Zilch\n",
    "* we have only a single data set\n",
    "* If there are large no. of data sets, we can combine them into a single large dataset\n",
    "* This would reduce the overall level of overfitting for a given model complexity"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
